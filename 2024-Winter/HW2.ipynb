{"cells":[{"cell_type":"markdown","source":["# Introducrtion to Machine Learning: Assignment #2\n","## Submission date: 21\\02\\2024, 23:59.\n","### Topics:\n","- Perceptron\n","- Logistic Regression\n","- Gradient Descent\n","- SVM\n","- Kernels"],"metadata":{"id":"aj440z9b98Zw"}},{"cell_type":"markdown","source":["Submitted by:\n","\n"," **Student 1 Name+ID\n","\n"," **Student 2 Name+ID"],"metadata":{"id":"em4OeZTD9-R2"}},{"cell_type":"markdown","source":["**Assignment Instruction:**\n","\n","· Submissions in pairs only.\n","\n","· The code must be reasonably documented\n","\n","· Try to keep the code as clean, concise, and short as possible\n","\n","· Your submission must be entirely your own. Any attempts of plagiarism (including ChatGPT) will lead to disciplinary actions.\n","\n","· You should save a copy of the notebook to your Drive and answer all the questions inside the notebook, at the designated cells. Only the notebook will be submitted in moodle (in `.ipynb` format).\n","\n","· If you wish to work in your IDE, make a `.py` copy of the notebook, but as you finish insert the script back to the matching cells of the notebook.\n","\n","**Important:** All plots, results and outputs should be included in the notebook as the cells' outputs (run all cells and do not clear the output)."],"metadata":{"id":"GQ-GUBWN9_JY"}},{"cell_type":"markdown","source":["<b>Note:</b> Pay attention to the labels in the the different datasets (0/1) and that some of the learned algorithms work with labels (-1/1)."],"metadata":{"id":"s2gkAVADW8hq"}},{"cell_type":"markdown","source":["## Question 1 - Logistic regression"],"metadata":{"id":"PJPkQ__X2pKK"}},{"cell_type":"markdown","source":["You will implement a claaisifer to predict wether client will buy a SUV car or not. <br/>\n","The prediction will be done by the client's age, gender and salary. <br/>\n","The label is 1 when bought and 0 otherwise."],"metadata":{"id":"xBRpSfg9-QCB"}},{"cell_type":"markdown","source":["import libraries"],"metadata":{"id":"KCZXWQdQ4P_u"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"011X9kCz4Prr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["plotting function"],"metadata":{"id":"JSM2DaiaA7Jl"}},{"cell_type":"code","source":["from mpl_toolkits.mplot3d import Axes3D\n","\n","def plot(data, labels, w, bias):\n","\n","  a, b, c = w[0], w[1], w[2]\n","  d = bias\n","\n","  # create a 3D scatter plot\n","  fig = plt.figure()\n","  ax = fig.add_subplot(111, projection='3d')\n","  ax.scatter(data[:, 0], data[:, 1], data[:, 2], c=labels, cmap='coolwarm')\n","\n","  xx, yy = np.meshgrid(range(-2, 2), range(-2, 2))\n","  z = (-a * xx - b * yy - d) * 1.0 / c\n","\n","  ax.plot_surface(xx, yy, z, alpha=0.4)\n","  ax.azim += 30\n","  ax.elev += 10\n","  #ax.view_init(elev=0, azim=90, roll=45)\n","\n","  # customize the plot\n","  ax.set_xlabel('X')\n","  ax.set_ylabel('Y')\n","  ax.set_zlabel('Z')\n","  plt.title('3D Scatter Plot with 2D Labels')\n","  plt.show()"],"metadata":{"id":"lUQIPFlGA6rO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Read the dataset and visualize it"],"metadata":{"id":"U6BK3TZh4SFN"}},{"cell_type":"code","source":["df = pd.read_csv('https://sharon.srworkspace.com/ml/datasets/hw2/suv_data.csv')\n","df.head(3)"],"metadata":{"id":"2jgr03q14VGi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After looking at the data, drop irrelevant features (such that have no learnability meaning) and make sure your remaining features are numeric.\n","\n"],"metadata":{"id":"DJpUwwNU_A1f"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"n3OJNwxXEu8f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot on the densities of the features, choose the scaler we will be using.<br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"DODlnBWl_h9m"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","# Implement here"],"metadata":{"id":"3XZQs2952rGD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split the data into 80% train samples and 20% test samples, with random state 42. <br/>\n","Split the train into real train (70%) and validation (30%) with random state 42. <br/>\n","Apply the scaler on the train, validation and test sets. <br/>\n","Remember: when scaling the test, it should use all the training data.<br/>\n","Tip: For minimizing the loss function, what labels did we look at? Are they the same here?"],"metadata":{"id":"rZduxr402r8j"}},{"cell_type":"code","source":["# Import scaling library\n","# Implement here"],"metadata":{"id":"KMWSncGU7VtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the function Logistic_Regression_via_GD(P,y,lr):\n","-\tInput: an np array ‘P’ of ‘n’ rows and ‘d’ columns, a label vector ‘y’ of ‘n’ entries and learning rate parameter ‘lr’.\n","-\tOutput: The function computes the output vector ‘w’ (and ‘b’) which minimzes the logistic regression cost function on ‘P’ and ‘y’. <br/>\n","\n","The implementation should be fully yours. Don't use library implementation! <br/>\n","It should be done by implementing Gradient descent (with ‘lr’ as the learning rate) to solve logistic regression. <br/>\n","\n","Tip: The gradients may be large, you can use $\\frac{1}{n}\\nabla{L}$ (which is the true empirical loss' gradient)\n","\n"],"metadata":{"id":"CBR_lRqe8N18"}},{"cell_type":"code","source":["def sigmoid(z):\n","  return 1/(1+np.exp(-z))\n","\n","# For now, ignore the lambda, you will need it later\n","def Logistic_Regression_via_GD(P,y,lr,lamda = 0):\n","  # Implement here"],"metadata":{"id":"5hgUwWPF8QiM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the function Predict(w,b,p):\n","-\tInput: an input vector x which represents a sample, a vector (numpy) ‘w’ and a number ‘b’.\n","-\tOutput: the class prediction for ‘p’ of the logistic regression model defined by ‘w’ and ‘b’.\n","\n","Note: It is your choice if use predictions of 1/-1 or 1/0. Make sure to adjust your choice to the true labels (for comparison)."],"metadata":{"id":"Zbue-ARwBVMJ"}},{"cell_type":"code","source":["def predict(x,w,b):\n","  # Implement here"],"metadata":{"id":"CqrZl4vfBvGB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Call ‘Logistic_Regression_via_GD(P,y,lr)’, where ‘P’ and ‘y’ are the training data and the corresponding labels. <br/>\n","Try to find the best lr for the learning task."],"metadata":{"id":"WG7pMuREAWO9"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"6zL-7lp0AGcW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the model on the test data, print the accuracy and plot hyperplane on the <u>test data</u> using ‘plot’ as defined above."],"metadata":{"id":"kM6uMyZ-CPNn"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"PEJA6WQUCOfy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To improve generalization, we use a tool that is called regularization. <br/>\n","In simple words,\n","$L_{\\text{loss}}(w)=L_{\\text{logistic-reg}}(w)+λ⋅‖w‖^2$. <br/>\n","Change ‘Logistic_Regression_via_GD’ according to that loss and find the best hyperparameter $λ$ using the performance on the <u>validation</u>.\n"],"metadata":{"id":"tPslEv2uC5s2"}},{"cell_type":"code","source":["lamads = np.arange(0, 5, 0.1)\n","for lamda in lamads:\n","  #Solve logistic regression with lamda\n","  print(f\"Valdation accuracy for lamda={lamda:.2f}: {accuracy * 100}%\")"],"metadata":{"id":"vnc8M1o9fD6R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Print final test accuracy"],"metadata":{"id":"k8G3Dkp3ns7L"}},{"cell_type":"code","source":["# Implement here\n","print(f\"Test accuracy: {accuracy * 100}%\")"],"metadata":{"id":"9GEctQn1nsla"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 2 - SVM"],"metadata":{"id":"JqHofE9ur3xq"}},{"cell_type":"markdown","source":["You are given dataset for binary classification in 2D and aim to build the best SVM classifier."],"metadata":{"id":"qqQbT0rRM-lQ"}},{"cell_type":"markdown","source":["import libraries"],"metadata":{"id":"da1lbeIPNVcW"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.svm import SVC"],"metadata":{"id":"50PLGF_RNQmN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["load npy file"],"metadata":{"id":"zvvK-Q796DK7"}},{"cell_type":"code","source":["import requests\n","from io import BytesIO\n","\n","def load_npy_file(url):\n","  response = requests.get(url)\n","  if response.status_code == 200:\n","    npy_data = np.load(BytesIO(response.content), allow_pickle=True).item()\n","    return npy_data\n","  else:\n","    return None"],"metadata":{"id":"g3a9t0k76D6g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["load the data"],"metadata":{"id":"wBn4WFZiNYwQ"}},{"cell_type":"code","source":["data_dict = load_npy_file('https://sharon.srworkspace.com/ml/datasets/hw2/svm_data_2d.npy')\n","\n","# Access the data as needed\n","X_train = data_dict['X_train']\n","y_train = data_dict['y_train']\n","X_val = data_dict['X_val']\n","y_val = data_dict['y_val']"],"metadata":{"id":"wxGONKCDvHAO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Look at the plot of the training data. <br/>\n","What _geometric_ shape could (almost) perfectly separate the data?<br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"WJSQ7JMWwvA1"}},{"cell_type":"code","source":["plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr')\n","plt.xlabel('X')\n","plt.ylabel('Y')\n","plt.title('Generated Train')\n","plt.show()"],"metadata":{"id":"vEgrPhvMwwVY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we want to separate the data using ellipsoid. <br/>\n","Its equation is:\n","$$Ax^2+By^2+Cx+Dy+e=0$$\n","Based on that equation, construct a mapping function into 4d space, such that the problem will become a linear ($w^t ϕ(x)+e=0$). <br/>\n","After the mapping, learn a linear classifier and print the hyperplane equation.<br/>\n","Note: after getting an output, I would recommend you to plot this equation on desmos, just to \"see it\"."],"metadata":{"id":"H5blMGXcOC1N"}},{"cell_type":"code","source":["new_features = # Implement here\n","\n","model = SVC(kernel='linear', C=10)\n","model.fit(new_features, y_train)\n","\n","# Get the hyperplane equation coefficients and intercept\n","coefficients = model.coef_[0]\n","intercept = model.intercept_\n","\n","# Print the hyperplane equation\n","equation_parts = []\n","for i in range(len(coefficients)):\n","    equation_parts.append(f\"({coefficients[i]:.3f} * X{i+1})\")\n","equation = \" + \".join(equation_parts) + f\" + ({intercept[0]:.3f})\"\n","\n","print(\"Hyperplane equation:\")\n","print(f\"  {equation}\")"],"metadata":{"id":"0OyItC8Cv0Dz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Complete the missing lines to get plots on train and test"],"metadata":{"id":"bSn-OzSoPNsW"}},{"cell_type":"code","source":["train_features = # Implement here\n","train_preds = # Implement here\n","train_acc = # Implement here\n","\n","val_features = # Implement here\n","val_preds = # Implement here\n","val_acc = # Implement here\n","\n","xx, yy = np.meshgrid(np.arange(-2, 2.2, 0.1), np.arange(-2, 2.2, 0.1))\n","data = np.c_[xx.ravel(), yy.ravel()]\n","\n","new_features = # Implement here the new features on 'data'\n","Z = # Implement here the predictions of data into 2 classes, using w, b you found\n","Z = Z.reshape(xx.shape)\n","\n","fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n","\n","# Plot the training data on the first subplot\n","axs[0].contourf(xx, yy, Z, alpha=0.8)\n","scatter1 = axs[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr')\n","axs[0].set_xlabel('X')\n","axs[0].set_ylabel('Y')\n","axs[0].set_title(f'Train dataset - {train_acc:.4f} accuracy')\n","\n","# Plot the validation data on the second subplot\n","axs[1].contourf(xx, yy, Z, alpha=0.8)\n","scatter2 = axs[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap='bwr')\n","axs[1].set_xlabel('X')\n","axs[1].set_ylabel('Y')\n","axs[1].set_title(f'Validation dataset - {val_acc:.4f} accuracy')\n","\n","plt.show()"],"metadata":{"id":"Ah59x2C9OyKZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You will try also two other kernels:\n","  - kernel='poly', degree=4, C=10.\n","  -\tkernel='rbf', gamma=1.5, C=10.\n","\n","Based on those two models, along with the ellipsoid, which model generalizes the best? <br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"qoIzr9VLPsv3"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","model = # Implement here\n","clf = model.fit(X_train, y_train)\n","\n","xx, yy = np.meshgrid(np.arange(-2, 2.2, 0.1), np.arange(-2, 2.2, 0.1))\n","xy = np.c_[xx.ravel(), yy.ravel()]\n","\n","P = model.decision_function(xy).reshape(xx.shape)\n","\n","fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n","\n","# Plot the training data on the first subplot\n","axs[0].contourf(xx, yy, P, alpha=0.8)\n","scatter1 = axs[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr')\n","axs[0].set_xlabel('X')\n","axs[0].set_ylabel('Y')\n","axs[0].set_title(f'Train dataset - {clf.score(X_train, y_train):.4f} accuracy')\n","\n","# Plot the validation data on the second subplot\n","axs[1].contourf(xx, yy, P, alpha=0.8)\n","scatter2 = axs[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap='bwr')\n","axs[1].set_xlabel('X')\n","axs[1].set_ylabel('Y')\n","axs[1].set_title(f'Validation dataset - {clf.score(X_val, y_val):.4f} accuracy')\n","\n","plt.show()"],"metadata":{"id":"IuyEg_B9_4lE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","model = # Implement here\n","clf = model.fit(X_train, y_train)\n","\n","xx, yy = np.meshgrid(np.arange(-2, 2.2, 0.1), np.arange(-2, 2.2, 0.1))\n","xy = np.c_[xx.ravel(), yy.ravel()]\n","\n","P = model.decision_function(xy).reshape(xx.shape)\n","\n","fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n","\n","# Plot the training data on the first subplot\n","axs[0].contourf(xx, yy, P, alpha=0.8)\n","scatter1 = axs[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr')\n","axs[0].set_xlabel('X')\n","axs[0].set_ylabel('Y')\n","axs[0].set_title(f'Train dataset - {clf.score(X_train, y_train):.4f} accuracy')\n","\n","# Plot the validation data on the second subplot\n","axs[1].contourf(xx, yy, P, alpha=0.8)\n","scatter2 = axs[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap='bwr')\n","axs[1].set_xlabel('X')\n","axs[1].set_ylabel('Y')\n","axs[1].set_title(f'Validation dataset - {clf.score(X_val, y_val):.4f} accuracy')\n","\n","plt.show()"],"metadata":{"id":"8sCdYxDyFuii"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use the validation dataset to tune better hyperparameter for rbf (gamma). Use C=10 as before."],"metadata":{"id":"qO2xBTlTQJG2"}},{"cell_type":"code","source":["gammas = # Add gammas to your choice\n","accs = []\n","\n","for gamma in gammas:\n","  # Implement here\n","\n","plt.figure(figsize=(15,6))\n","plt.plot(gammas, accs, color='red')\n","plt.xlabel('gamma')\n","plt.ylabel('accuracy')\n","plt.title('Tuning')\n","plt.xticks(gammas)\n","plt.show()"],"metadata":{"id":"QKCt1ZLir4M4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Questions:\n","- Without coding, only from the plot, will the accuracy change using the best $\\gamma$ you found? Why?\n","- Why did we use C=10? <br/>\n","\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"Fvz0DKtC-mG3"}},{"cell_type":"markdown","source":["## Question 3 - Perceptron"],"metadata":{"id":"NKWjqVSG-hZG"}},{"cell_type":"markdown","source":["You will implement perceptron via gradient descent.\n","\n","The data set is taken from Andrew course on Coursera. The data consists of marks from two exams for 100 applicants and binary labels: 1 when applicant was admitted and 0 otherwise.\n","\n","The objective is to build a classifier that can predict whether an application will be admitted to the university or not. We <u>will not</u> use test set, but only analyze the training.<br/>\n","\n","The data is available here: https://sharon.srworkspace.com/ml/datasets/hw2/exams.csv"],"metadata":{"id":"E9i0j6UYRlYA"}},{"cell_type":"markdown","source":["import libraries"],"metadata":{"id":"WcTtpyuLRnos"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from IPython.display import display, clear_output"],"metadata":{"id":"S9McjsXS-inp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the data, plot it, and make sure you understand its shape."],"metadata":{"id":"KhiHyOHbWCOU"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"CnxdtJ9H-uMu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prepare the data by scaling it into 0-1 range (via MinMaxScaler)"],"metadata":{"id":"qeG9D8GYWTgG"}},{"cell_type":"code","source":["# Import library\n","# Implement here"],"metadata":{"id":"GF3T2-DkDvef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the function perceptron(data, labels, lr) which gets the train data along with labels and returns the weight vector learned by perceptron. <br/>\n","Note: if you choose to use iterations, don’t use more than 10,000 iterations.<br/>\n","\n"],"metadata":{"id":"tQAuuHqiWjFO"}},{"cell_type":"code","source":["def perceptron(data, labels, lr = 1):\n","  # Implement here"],"metadata":{"id":"o7VtxRETAD-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot(train, labels, w, bias, show=True):\n","\t# Create a figure and axis object\n","\tfig, ax = plt.subplots()\n","\n","\tc0 = train[labels == -1]\n","\tc1 = train[labels == 1]\n","\n","\t# Plot the data\n","\tax.scatter(c0[:,0], c0[:,1], c='red')\n","\tax.scatter(c1[:,0], c1[:,1], c='blue')\n","\n","\ta, b, c = w[0], w[1], bias\n","\n","\t# Compute the slope and y-intercept of the line\n","\tm = -a / b\n","\tb = -c / b\n","\n","\t# Generate some x values for the plot\n","\tx = np.arange(np.min(train[:,0]), np.max(train[:,0]), 0.1)\n","\n","\t# Compute the corresponding y values using the equation of the line\n","\ty = m * x + b\n","\n","\t# Plot the line\n","\tplt.plot(x, y)\n","\n","\t# Add axis labels and title\n","\tax.set_xlabel('X-axis')\n","\tax.set_ylabel('Y-axis')\n","\n","\tpreds = np.sign(np.dot(train, w)+bias)\n","\tacc = np.count_nonzero(labels == preds) / len(labels)\n","\n","\tax.set_title(f'Train accuracy is {acc}')\n","\tax.set_xlim(-0.1, 1.1)\n","\tax.set_ylim(-0.1, 1.1)\n","\n","\tif show:\n","\t\tplt.show()"],"metadata":{"id":"i-Vs-hdxUtck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Call the perceptron and plot function with your data."],"metadata":{"id":"edtNeU8_XjiL"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"AfQtjdHxCgUq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Upgrade the \"perceptron\" function such that it will return array of weight vectors, from each iteration during the process. <br/>\n","Call the \"plot_anim\" function and wait untill you see approximate converge <br/>\n","- Do you really need many iterations?\n","- Does perceptron converge on this dataset? <br/>\n","\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"ewGipk_3V27p"}},{"cell_type":"code","source":["ws = perceptron(...)    # Implement here\n","\n","def plot_anim(ws):\n","  for ww in ws:\n","    plt.clf()\n","    plot(data, labels, ww[:-1], ww[-1], False)\n","    display(plt.gcf())\n","    clear_output(wait=True)  # Clear the previous plot\n","\n","plot_anim(ws)"],"metadata":{"id":"L1E72lUYV5sF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will extend the theory of convergence in perceptron to non-separable case:\n","Let ${\\{x_i\\} }_{i=1}^n$ be the training set and $R$ such that $∀i:‖x_i ‖≤R$.<br/>\n","The deviation of each sample is defined by $d_i= \\max \\{0,1-y_i (w^t x_i) \\}$. <br/>\n","Let $D=‖d‖$, then the number of mistakes of the perceptron algorithm is upper bounded by $2(R+D)^2$.<br/>\n","Use this theorem to find (much) smaller number of iterations, use that exact number as upper bound and <u>report the train accuracy</u>.\n","Did it improve?<br/>\n","<font color='red'>Write here your answer and explain it</font>\n"],"metadata":{"id":"ish9lf0mX-GF"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"BMvFhAdINmpY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remark: actually, $$d_i=\\max \\{ 0,γ-y_i (w^t x_i ) \\}$$ and the bound is $$\\left(\\frac{R+D}{\\gamma}\\right)^2$$ where $$γ=\\min⁡ \\{y_i w^t x_i\\}$$\n","By better measurement of the radius, we can get an even better bound. Its it not required here though."],"metadata":{"id":"_Qct3ixHgmEF"}}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["PJPkQ__X2pKK","JqHofE9ur3xq","NKWjqVSG-hZG"],"mount_file_id":"1Kr1hAcsz3nQe5ce3vdzAXXyH9YeUk5YY","authorship_tag":"ABX9TyPgt6raA1xKPhZp8r3JTrS8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}