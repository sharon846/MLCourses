{"cells":[{"cell_type":"markdown","source":["# Introducrtion to Machine Learning: Assignment #1\n","## Submission date: 28\\5\\2024, 23:59.\n","### Topics:\n","- Naïve bayes\n","- Gaussian Bayes\n","- Parzen Windows\n","- KNN"],"metadata":{"id":"WxQj_LVp7onx"}},{"cell_type":"markdown","source":["Submitted by:\n","\n"," **Student 1 Name+ID\n","\n"," **Student 2 Name+ID"],"metadata":{"id":"Mih4qIYq7so3"}},{"cell_type":"markdown","source":["**Assignment Instruction:**\n","\n","· Submissions in pairs only.\n","\n","· Try to keep the code as clean, concise, and short as possible\n","\n","· If you wish to work in your IDE, you can, but you **must**,  insert the script back to the matching cells of the notebook and run the code. <br/>Only the notebook will be submitted in moodle (in `.ipynb` format).\n","\n","· <font color='red'>Please write your answers to question in red</font>.\n","\n","**Important:** All plots, results and outputs should be included in the notebook as the cells' outputs (run all cells and do not clear the output). <br/>\n","\n","**Important:** Your submission must be entirely your own. Any attempts of plagiarism (including ChatGPT) will lead to grade 0 and disciplinary actions.\n"],"metadata":{"id":"D3nU_S097vG5"}},{"cell_type":"markdown","source":["## Customizing Colab\n","This is an optional section for you convenience:<br/>\n","Go to Tools -> Settings -> editor<br/>\n","There, you can adjust fonts, add line numbers, change indentations."],"metadata":{"id":"hp1AkkIy-W_c"}},{"cell_type":"markdown","source":["## Question 1 - Bayesian Classification Assuming Gaussian distribution\n","You are requested by the Central Bank of America to detect between three types of bankotes: fake, fine, and good. The dataset consists of 5 features which were obtained from the digitized images of banknotes.<br/>\n","Since the data is continuous, you will implement Gaussian bayes and compare to Gaussian naïve bayes.\n","\n","\n"],"metadata":{"id":"dIRNFk32EwZG"}},{"cell_type":"markdown","source":["import libraries"],"metadata":{"id":"X2RcDlZcEwZQ"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"metadata":{"id":"HI8EMI0KEwZQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the wine dataset from https://sharon.srworkspace.com/ml/datasets/hw1/banknote_authentication.csv"],"metadata":{"id":"ZC76_8irEwZR"}},{"cell_type":"code","source":["# Implement here\n","\n","print(df.shape)\n","df.head(3)"],"metadata":{"id":"85RuF65OEwZR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check if there is even potential for gaussian assumption to work here, by plotting the density of the features (without target label!), using plotting for data frames. <br/>\n","Should gaussian bayes work here? <br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"PttbpOWzEwZR"}},{"cell_type":"code","source":["df.plot(kind='density', subplots=True, layout=(2,3), figsize=(15, 5), sharex=False)\n","plt.show()"],"metadata":{"id":"3dCLDfyAEwZR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convert the data to numpy and split the data to 80% training and 20% test with random state of 33. Make sure to maintain the dataset balanced, using stratify=y, in train_test_split method.\n","<br/>Note that the data frame currently includes the labels as well."],"metadata":{"id":"js0HJ50nEwZR"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"pHeLEAo2EwZR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the functions below. <br/>Both get test data $X\\in\\mathbb{R}^{n\\times d}$ and returns the predicted classes (vector sized n), but the naïve bayes assumes that the features are independent.<br/>\n","Hint for efficient implementation: you don't need more than one loop, use numpy!"],"metadata":{"id":"Ydcyd7i3EwZS"}},{"cell_type":"code","source":["def classify_point_gaussian_bayes(test_data):\n","  # Implement here\n","\n","def classify_point_gaussian_naive_bayes(test_data):\n","  # Implement here"],"metadata":{"id":"bIhawfwSEwZS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For both GB and GNB, we will look at train vs test. Answer the following:\n","- Which model achieved better learning? Why?\n","- Could the other model be sometimes better? How, for example?\n","\n","<font color='red'>Write here your answers and explain</font>\n","<br/>Hint: Use the next section, for visualization the correlation matrix on the train.  \n"],"metadata":{"id":"vJRV-LE7EwZS"}},{"cell_type":"code","source":["# Reminder: success rate is the precentage of correctly classified data within the number of all data in the test set.\n","\n","dict1 = {'GB': [], 'GNB': []}\n","\n","accs = classify_point_gaussian_bayes(x_train)\n","dict1['GB'].append(np.count_nonzero(accs == y_train) / len(y_train))\n","\n","accs = classify_point_gaussian_bayes(x_test)\n","dict1['GB'].append(np.count_nonzero(accs == y_test) / len(y_test))\n","\n","accs = classify_point_gaussian_naive_bayes(x_train)\n","dict1['GNB'].append(np.count_nonzero(accs == y_train) / len(y_train))\n","\n","accs = classify_point_gaussian_naive_bayes(x_test)\n","dict1['GNB'].append(np.count_nonzero(accs == y_test) / len(y_test))\n","\n","df = pd.DataFrame(dict1, columns=['GB', 'GNB'], index=['train', 'test'])\n","print(df)"],"metadata":{"id":"M5SwEIv1HEXC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Its not enough! We want to get even better test results. <br/>\n","Please look at the train correlation matrix.\n","- Choose one feature to remove and explain why you chose it.\n","- If we continue to remove features, what do you expect to happend with the train error?\n","\n","<font color='red'>Write here your answers and explain them</font>"],"metadata":{"id":"wCJQpXvAJCd8"}},{"cell_type":"code","source":["# Implement here. you may use the code from the tutorial.\n","# You can obtain the correlation matrix using numpy"],"metadata":{"id":"N1cOIzcvJZ1S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Adjust the data according to your decision and print again the train and test for both models."],"metadata":{"id":"rls9T0EoJ6r2"}},{"cell_type":"code","source":["# Change here the train and the test according to the feature you chose to remove\n","\n","dict1 = {'GB': [], 'GNB': []}\n","\n","accs = classify_point_gaussian_bayes(x_train)\n","dict1['GB'].append(np.count_nonzero(accs == y_train) / len(y_train))\n","\n","accs = classify_point_gaussian_bayes(x_test)\n","dict1['GB'].append(np.count_nonzero(accs == y_test) / len(y_test))\n","\n","accs = classify_point_gaussian_naive_bayes(x_train)\n","dict1['GNB'].append(np.count_nonzero(accs == y_train) / len(y_train))\n","\n","accs = classify_point_gaussian_naive_bayes(x_test)\n","dict1['GNB'].append(np.count_nonzero(accs == y_test) / len(y_test))\n","\n","df = pd.DataFrame(dict1, columns=['GB', 'GNB'], index=['train', 'test'])\n","print(df)"],"metadata":{"id":"HV6kqsLDJ68S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the boundaries plotting for (scaled) train and test with gaussian bayes.<br/>It will show the decision boundaries as saw in the lectures."],"metadata":{"id":"LdFxOkkVEwZS"}},{"cell_type":"code","source":["# Essential for the visualization\n","\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","x_train = sc.fit_transform(x_train)\n","x_test = sc.transform(x_test)"],"metadata":{"id":"2E2zk67-EwZT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Train Decision Boundaries of Gaussian Naive Bayes (2D)\n","\n","from sklearn.decomposition import PCA\n","from tqdm import tqdm\n","\n","# Reduce the dimensionality of the data to 2 using PCA\n","pca = PCA(n_components=2)\n","X_reduced = pca.fit_transform(x_train)\n","\n","# Create a grid of points for visualization in the reduced 2D space\n","x_min, x_max = X_reduced[:, 0].min() - 1, X_reduced[:, 0].max() + 1\n","y_min, y_max = X_reduced[:, 1].min() - 1, X_reduced[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n","\n","# Use the GNB model to predict class labels for the grid points in the original 13D space\n","grid_points = pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()])\n","print(grid_points.shape)\n","Z = classify_point_gaussian_naive_bayes(grid_points)\n","Z = Z.reshape(xx.shape)\n","\n","# Plot the decision boundaries and the data points in the reduced 2D space\n","plt.contourf(xx, yy, Z, alpha=0.8)\n","scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_train, cmap=plt.cm.RdYlBu)\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","\n","# Add a legend\n","handles, labels = scatter.legend_elements()\n","plt.legend(handles, labels, title='Classes')\n","\n","plt.title('Train Decision Boundaries of Gaussian Naive Bayes (2D)')\n","plt.show()"],"metadata":{"cellView":"form","id":"vu6rj4QXEwZT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Train Decision Boundaries of Gaussian Naive Bayes (2D)\n","\n","from sklearn.decomposition import PCA\n","from tqdm import tqdm\n","\n","# Reduce the dimensionality of the data to 2 using PCA\n","X_reduced = pca.transform(x_test)\n","\n","# Create a grid of points for visualization in the reduced 2D space\n","x_min, x_max = X_reduced[:, 0].min() - 1, X_reduced[:, 0].max() + 1\n","y_min, y_max = X_reduced[:, 1].min() - 1, X_reduced[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n","\n","# Use the GNB model to predict class labels for the grid points in the original 13D space\n","grid_points = pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()])\n","print(grid_points.shape)\n","Z = classify_point_gaussian_naive_bayes(grid_points)\n","Z = Z.reshape(xx.shape)\n","\n","# Plot the decision boundaries and the data points in the reduced 2D space\n","plt.contourf(xx, yy, Z, alpha=0.8)\n","scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_test, cmap=plt.cm.RdYlBu)\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","\n","# Add a legend\n","handles, labels = scatter.legend_elements()\n","plt.legend(handles, labels, title='Classes')\n","\n","plt.title('Test Decision Boundaries of Gaussian Naive Bayes (2D)')\n","plt.show()"],"metadata":{"cellView":"form","id":"BSVMZb6NEwZT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 2 - Naive Bayes\n","\n","In this problem, you’ll implement a basic Naïve Bayes classifier, and use it to predict an emotion by description. <br/>\n","You will have to classify the sentences into 6 categories, <b>but could be any number.</b><br/>\n","The categories are {'sadness', 'joy', 'love', 'fear', 'anger', 'ambiguous'}. <br/>\n","\n","<b>Warning:</b> I haven't personally looked at all the data here. Even though the data is taken from a ML databases site, accept my apologies if there are any offensive sentence.\n"],"metadata":{"id":"acuxlnhGm9BC"}},{"cell_type":"markdown","source":["import libarires"],"metadata":{"id":"BuhWspAV1n-l"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import math\n","from sklearn.feature_extraction.text import CountVectorizer"],"metadata":{"id":"AtBhju1K1qXh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the function. It reads all tweets from file and returns the following data structures: <br/>\n","•\ttexall - list of documents; each entry corresponds to a tweet which is list of words. <br/>\n","•\tlbAll list of tweets' labels.<br/>\n","•\tvoc - set of all distinct words in the file.<br/>\n","•\tcat - set of tweets categories.\n"],"metadata":{"id":"SwwN8oVTn-HO"}},{"cell_type":"code","source":["def readTrainData(file_name):\n","  df = pd.read_csv(file_name)\n","  # Implement here\n","  return texAll, lbAll, voc, cat"],"metadata":{"id":"_zZYY_brn9Vx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the function, which computes and returns the probabilities (on the train set):<br/>\n","- $P_w$ - a matrix of class-conditional probabilities, $p(x|w_i)$\n","- $P$ - a vector of class priors, $p(w_i)$\n","\n","Make sure you deal with the case of word that appears in voc but not in class $w$."],"metadata":{"id":"6xUbLY2Tol7u"}},{"cell_type":"code","source":["def learn_NB_text():\n","  # Implement here\n","\treturn Pw, P"],"metadata":{"id":"uYiXzZCIow9p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement fhe function that classifies all tweets from the test set and computes the success rate.<br/>\n","Iterate over all tweets of test and for each tweet find the most probable category.\n","<br/><br/>\n","Note1: Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. <br/>Class with highest final un-normalized log probability score is still the most probable.\n"],"metadata":{"id":"SkmMTneCpTm-"}},{"cell_type":"code","source":["def ClassifyNB_text(Pw, P):\n","\t# Implement here"],"metadata":{"id":"-epXTXF5EHtx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Read the files"],"metadata":{"id":"LMZXLXTbp84V"}},{"cell_type":"code","source":["TRAIN_FILE = 'https://sharon.srworkspace.com/ml/datasets/hw1/emotions_train.csv'\n","TEST_FILE = 'https://sharon.srworkspace.com/ml/datasets/hw1/emotions_test.csv'\n","\n","texAll_train, lblAll_train, voc, cat = readTrainData(TRAIN_FILE)\n","\n","# cats must be the same at train and test\n","# voc of test is irrelevant - we already trained on other voc.\n","texAll_test, lblAll_test, _, __ = readTrainData(TEST_FILE)"],"metadata":{"id":"wZa-wSjvZK2s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train the model, classify it on the test and report the success rate"],"metadata":{"id":"m1ss4d_dqyk5"}},{"cell_type":"code","source":["Pw, P = learn_NB_text()\n","sum_right = ClassifyNB_text(Pw, P)\n","print(sum_right)"],"metadata":{"id":"u5_sN3V-GOah"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 3 - KNN\n","You want to detect stars into 6 types of stars by measuring their properties. <br/> NASA gave you their dataset, including temperature, color, Spectral_Class and more. <br/> In addition, you aim to compare different distance metric to determine which one is the best for this data."],"metadata":{"id":"MsyhpIFXgWbo"}},{"cell_type":"markdown","source":["import libaries"],"metadata":{"id":"jI5R52AwqOdA"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"],"metadata":{"id":"IrdvlliQgXpE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the data, print the first three rows<br/>\n","https://sharon.srworkspace.com/ml/datasets/hw1/Stars.csv"],"metadata":{"id":"YMt83YEaqPqf"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"rAGt7yWzn-pj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convert categorial features to discerete values"],"metadata":{"id":"RMJVaS8DqRBB"}},{"cell_type":"code","source":["colors = df['Color'].unique()\n","for idx, color in enumerate(colors):\n","  df['Color'] = df['Color'].replace({color: idx})\n","\n","spec_class = df['Spectral_Class'].unique()\n","for idx, spec in enumerate(spec_class):\n","  df['Spectral_Class'] = df['Spectral_Class'].replace({spec: idx})\n","df.head(3)"],"metadata":{"id":"nlPbLdxqooAy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check the correlation matrix between the features. <br/>\n","Which distance metric do you expect to work better: Euclidean distance, of the Mahalanobis distance? <br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"685jpA45qS_D"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"vrb2NRL6p6gC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split the data to 90% train and 10% test, with random state 42. <br/>\n","Afterwards, split the train into 80% train and 20% validation, also with random state of 42<br/>\n","Make sure to maintain the dataset balanced, using stratify=y, in train_test_split method. <br/> You can check the balance using df.value_counts()."],"metadata":{"id":"4akX_fZnunkb"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"pzBXvhu9ukzG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the function 'Euclidian'. <br/>\n","This function gets train and test datasets ($m\\times d, n\\times d$) and returns the distance metric sized $m \\times n$, based on euclidian distance metric\n"],"metadata":{"id":"CfUnWaYiu59l"}},{"cell_type":"code","source":["def Euclidean(test, data):\n","  # Implement here\n","\n","def Mahalanobis(test, data):\n","  distances = np.zeros((test.shape[0], data.shape[0]))\n","  covariance_matrix_data = np.cov(data, rowvar=False)\n","\n","  # Calculate the Mahalanobis distances\n","  for i in range(test.shape[0]):\n","      for j in range(data.shape[0]):\n","          diff =  test[i] - data[j]\n","          distances[i, j] = np.sqrt(np.dot(np.dot(diff, np.linalg.inv(covariance_matrix_data)), diff.T))\n","  return distances"],"metadata":{"id":"Werkv3zoztln"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the function kNN_classify that returns array sized m, which are the predictions for the m test samples."],"metadata":{"id":"_U-RaTeOvYOe"}},{"cell_type":"code","source":["def kNN_classify(data, labels, test, k, metric='Euclidian'):\n","  arguments = (test, data)\n","  distances = eval(f'{metric}(*arguments)')   #returns np[][] |test| X |data| by the given metric.\n","  # Implement here"],"metadata":{"id":"1wvWALopvZd5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Look at the plots for different k values. Note that the Mahalanobis distance metric is already implemented to you above. <br/>Run the following code and compare the performance of Mahalanobis vs Euclidian, specifically as function of k. <br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"oBOw9dqxwRLm"}},{"cell_type":"code","source":["metrics = ['Euclidean', 'Mahalanobis']\n","fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n","\n","for idx, metric in enumerate(metrics):\n","\n","  ks = np.arange(1, 41, 2)\n","  accs = []\n","  for k in ks:\n","    c = kNN_classify(X_train, y_train, X_test, k, metric)\n","    accs.append()   # Implement here\n","\n","  axs[idx % 2].plot(ks, accs, color='red')\n","  axs[idx % 2].set_xlabel('k')\n","  axs[idx % 2].set_ylabel('accuracy')\n","  axs[idx % 2].set_title(metric)\n","  axs[idx % 2].set_xticks(ks)\n","plt.show()"],"metadata":{"id":"Wp_CDXB4whVs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we implement the $L_p$ norm distance metric. Reminder:\n","$$ d(x,y)=||x-y||_p=\\left(\\sum_{i=1}^d |x_i-y_i|^p\\right)^{\\frac{1}{p}} $$\n","Remark: $L_1$ is the Manhattan distance and $L_2$ is the Euclidian."],"metadata":{"id":"lcCrV-VjG7J6"}},{"cell_type":"code","source":["def Minkowski(test, data, p):\n","  # Implement here"],"metadata":{"id":"B4y7DVy_G-at"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Find the hyperparameter p that fits best to this problem (between 1-10)"],"metadata":{"id":"t3LEiAVxHEPi"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"uuCyPfvSHJZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For that p, print the accuracy on the test as function of k. Is it really better  than the first two matrices? <br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"_X5w2DWbHOXv"}},{"cell_type":"code","source":["ks = np.arange(1, 30, 2)\n","accs = []\n","best_p = None       # You may change the usage of p according to your previous implementation\n","\n","for k in ks:\n","    c = kNN_classify(X_train, y_train, X_test, k, 'Minkowski')\n","    accs.append()   # Implement here\n","\n","plt.plot(ks, accs)\n","plt.xticks(ks)\n","plt.show()"],"metadata":{"id":"obaDY5IxHO7x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Thats it!\n","If you choose to continue, 5 pts bonus!"],"metadata":{"id":"mGtnsYXBH-MV"}},{"cell_type":"markdown","source":["We are going to classify the fashion MNIST data.<br/>\n","First, load the the train and test from:\n","- https://sharon.srworkspace.com/ml/datasets/hw1/fashion-mnist_train.csv\n","- https://sharon.srworkspace.com/ml/datasets/hw1/fashion-mnist_test.csv\n","\n","Seconly, convert it to numpy and obtain the train and test data & labels"],"metadata":{"id":"FHxbC717H_qH"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"ivMxHfJZI8bm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["dispaly some random image"],"metadata":{"id":"CtLsbrZtJhxy"}},{"cell_type":"code","source":["idx = np.random.randint(len(X_train))\n","plt.imshow(X_train[idx].reshape(28,28), cmap='gray')\n","plt.show()"],"metadata":{"id":"1JHwU5bdJima"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use some google to search for appropriate distance metric that should classify correctly images. From what you read, explain yourselves, why is it a good distance metric?<br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"-GxIuPvZJlUI"}},{"cell_type":"markdown","source":["Now, use the sklearn package of KNN with the metric you chose and print a graph of accuracy on the test, as function of k.<br/>\n","Were you correct?"],"metadata":{"id":"nUULkrBFJl2R"}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","# Implement here"],"metadata":{"id":"PG0OCMufJoKQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 4 - Parzen Windoes\n","See attached PDF in Moodle"],"metadata":{"id":"TjFRb665EE2Z"}}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["dIRNFk32EwZG","acuxlnhGm9BC","MsyhpIFXgWbo","mGtnsYXBH-MV"],"mount_file_id":"10Aq7iM8jGdN7GozuaSoIs4Br7VZvdWbC","authorship_tag":"ABX9TyMfc+3AQY0QRwDa8M1i8SA7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}