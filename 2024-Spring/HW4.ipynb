{"cells":[{"cell_type":"markdown","source":["# Introducrtion to Machine Learning: Assignment #4\n","## Submission date: 31\\07\\2024, 23:55.\n","### Topics:\n","- Ensemble methods\n","- AdaBoost\n","- PCA\n","- LDA\n","- K means clustering"],"metadata":{"id":"iBHBci_i2IgA"}},{"cell_type":"markdown","source":["Submitted by:\n","\n"," **Student 1 Name+ID\n","\n"," **Student 2 Name+ID"],"metadata":{"id":"NGo4MrnG2NXa"}},{"cell_type":"markdown","source":["**Assignment Instruction:**\n","\n","· Submissions in pairs only.\n","\n","· Try to keep the code as clean, concise, and short as possible\n","\n","· If you wish to work in your IDE, you can, but you **must**,  insert the script back to the matching cells of the notebook and run the code. <br/>Only the notebook will be submitted in moodle (in `.ipynb` format).\n","\n","· <font color='red'>Please write your answers to question in red</font>.\n","\n","**Important:** All plots, results and outputs should be included in the notebook as the cells' outputs (run all cells and do not clear the output). <br/>\n","\n","**Important:** Your submission must be entirely your own. Any attempts of plagiarism (including ChatGPT) will lead to grade 0 and disciplinary actions.\n"],"metadata":{"id":"eietFcHy2Kr2"}},{"cell_type":"markdown","source":["## Question 1 - Bagging\n","In HW3, you helped Charles Darvin with regression of abalone problem and now, you will try combining multiple regression models instead of just one, hopefully for a better result."],"metadata":{"id":"Kh7ZuCb6r9Fs"}},{"cell_type":"code","source":["# import libraries\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","scaler = StandardScaler()"],"metadata":{"id":"tDcWtpOxuaqd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load the data, transform it\n","\n","df = pd.read_csv('https://sharon.srworkspace.com/ml/datasets/hw3/abalone.csv', header=None)\n","data = df.to_numpy()\n","\n","train, test = train_test_split(data, test_size=0.2, random_state=21)\n","\n","X_train, y_train = train[:,:-1], train[:, -1]\n","X_test, y_test = test[:,:-1], test[:, -1]\n","\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"K9dL92Wbuhb5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Your task is to complete the following bagging model. Specifically:\n","- fit(self, data, targets) - train n_estimators regressors, each with data in size n=len(data) that is drawn from the original data, with repititions.\n","- predict(self, test) - predict the result for all the regressors as learned.\n","\n"],"metadata":{"id":"7Zs4Gjueu0CY"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","\n","class LinRegCombiner():\n","    def __init__(self, n_estimators):\n","        self.n_estimators = n_estimators\n","\n","    def fit(self, data, targets):\n","        self.regressors = []\n","\n","        for _ in range(self.n_estimators):\n","            # Implement here\n","\n","    def predict(self, test):\n","        preds = []\n","        # Implement here\n","\n","    def score(self, test, targets):\n","        temp = self.predict(test)\n","        return np.mean((targets - temp) ** 2)"],"metadata":{"id":"mfx59vQd0J8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run for 100 estimators\n","\n","obj = LinRegCombiner(n_estimators=100)\n","obj.fit(X_train, y_train)\n","\n","mse = obj.score(X_train, y_train)\n","print(f'MSE train = {mse}')\n","\n","mse = obj.score(X_test, y_test)\n","print(f'MSE test = {mse}')"],"metadata":{"id":"P3EVI4T10IWj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, implement the same but using ridge regression"],"metadata":{"id":"8KxmUKpW34YX"}},{"cell_type":"code","source":["from sklearn.linear_model import Ridge\n","from sklearn.base import BaseEstimator, RegressorMixin\n","\n","class RidgeLinRegCombiner(BaseEstimator, RegressorMixin):\n","    def __init__(self, n_estimators, alpha=1.0):\n","        self.n_estimators = n_estimators\n","        self.alpha = alpha\n","\n","    def fit(self, data, targets):\n","        self.regressors = []\n","\n","        for _ in range(self.n_estimators):\n","            # Implement here\n","\n","    def predict(self, test):\n","        preds = []\n","        # Implement here\n","\n","    def score(self, test, targets):\n","        temp = self.predict(test)\n","        return np.mean((targets - temp) ** 2)"],"metadata":{"id":"HSCAHyo4375e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tune the hyperparameters for RidgeLinRegCombiner."],"metadata":{"id":"WRgpTlHh4C-X"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"aaaGgQip4pg4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Print both MSEs on train and test.\n","Which model is better for our problem? <br/>\n","<font color='red'>Write your answer here and explain it</font>"],"metadata":{"id":"RpYoylPfK-A0"}},{"cell_type":"code","source":["# Run for n estimators and alpha you found\n","\n","obj = RidgeLinRegCombiner(n_estimators=n, alpha=alpha)\n","obj.fit(X_train, y_train)\n","\n","mse = obj.score(X_train, y_train)\n","print(f'MSE train = {mse}')\n","\n","mse = obj.score(X_test, y_test)\n","print(f'MSE test = {mse}')"],"metadata":{"id":"kvg4j7LzKJHb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 2 - Clustering\n","\n","We learned in the tutorials about partitional clustering and specifically – k means algorithm. <br/>\n","In this question you will implement it and see some nice applications."],"metadata":{"id":"wqPk-EK5tBJT"}},{"cell_type":"markdown","source":["import libraries"],"metadata":{"id":"KTd61ral4Ju3"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"NA919a0U4MFo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Complete the missing implementation of Kmeans. Since there are k clusters, we will label each point with {0,..,k-1}."],"metadata":{"id":"7LiNstqG3peu"}},{"cell_type":"code","source":["class Kmeans:\n","\n","\tdef __init__(self, n_clusters, max_iter=100, random_state=123):\n","\t\tself.n_clusters = n_clusters\n","\t\tself.max_iter = max_iter\n","\t\tself.random_state = random_state\n","\n","\tdef initialize_centroids(self, X):\n","\t\tnp.random.RandomState(self.random_state)\n","\t\trandom_idx = np.random.permutation(X.shape[0])\n","\t\tcentroids = X[random_idx[:self.n_clusters]]\n","\t\treturn centroids\n","\n","\tdef reassign_centroids(self, X, labels):\n","\t\tcentroids = np.zeros((self.n_clusters, X.shape[1]))\n","\t\t# Implement here\n","\t\treturn centroids\n","\n","\tdef compute_distance(self, X, centroids):\n","\t\tdistance = np.zeros((X.shape[0], self.n_clusters))\n","\t\tfor k in range(self.n_clusters):\n","\t\t\trow_norm = np.linalg.norm(X - centroids[k, :], axis=1)\n","\t\t\tdistance[:, k] = np.square(row_norm)\n","\t\treturn distance\n","\n","\tdef find_closest_cluster(self, distance):\n","\t\treturn np.argmin(distance, axis=1)\n","\n","\tdef compute_sse(self, X, labels, centroids):\n","\t\tdistance = np.zeros(X.shape[0])\n","\t\tfor k in range(self.n_clusters):\n","\t\t\tdistance[labels == k] = np.linalg.norm(X[labels == k] - centroids[k], axis=1)\n","\t\treturn np.sum(np.square(distance))\n","\n","\tdef fit(self, X):\n","\t\tself.centroids = self.initialize_centroids(X)\n","\t\tfor i in range(self.max_iter):\n","\t\t\told_centroids = self.centroids\n","\t\t\t# For each point, calculate distance to all k clustes.\n","\t\t\tself.labels =\t# Assign the labels with closest distance' cluster.\n","\t\t\tself.centroids = # Update the centroids\n","\t\t\tif np.all(old_centroids == self.centroids):\n","\t\t\t\tbreak\n","\t\tself.error = self.compute_sse(X, self.labels, self.centroids)\n","\n","\tdef predict(self, X):\n","\t\tdistance = self.compute_distance(X, self.centroids)\n","\t\treturn self.find_closest_cluster(distance)"],"metadata":{"id":"Y8ACpogUs4ux"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load exams data, convert to numpy and plot it"],"metadata":{"id":"01SMbF1T6UVM"}},{"cell_type":"code","source":["db = pd.read_csv('https://sharon.srworkspace.com/ml/datasets/hw4/exams.csv', header=None).to_numpy()\n","data, labels = db[:,:-1], db[:,-1]\n","\n","plt.scatter(data[:, 0], data[:, 1])\n","plt.show()"],"metadata":{"id":"GVLM73H9vb69"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We are going to divide the data into 2 clusters. <br/>\n","Define Kmeans object and fit the data."],"metadata":{"id":"i3yHa5ap6k3W"}},{"cell_type":"code","source":["clust = Kmeans(n_clusters=2)\n","clust.fit(data)\n","\n","# This code plots the clustered data with centroids\n","labels = clust.labels\n","centroids = clust.centroids\n","\n","c0 = data[labels == 0]\n","c1 = data[labels == 1]\n","\n","plt.scatter(c0[:,0], c0[:,1], c='green', label='cluster 1')\n","plt.scatter(c1[:,0], c1[:,1], c='blue', label='cluster 2')\n","plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=200, c='black', label='centroid')\n","plt.legend()\n","\n","plt.show()"],"metadata":{"id":"DV8XrX6AvdH-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use the Elbow Method to choose another number of centroids between 1-10. <br/>\n","<font color='red'>Explain your choice</font>"],"metadata":{"id":"Vn-tPolL8564"}},{"cell_type":"code","source":["sse = []\n","list_k = list(range(1, 11))\n","\n","for k in list_k:\n","  sse.append(error_of_current_clustering)\n","\n","'''Plot sse against k'''\n","plt.figure(figsize=(6, 6))\n","plt.plot(list_k, sse, '-o')\n","plt.xlabel(r'Number of clusters *k*')\n","plt.ylabel('Sum of squared distance')\n","plt.show()"],"metadata":{"id":"-3ebxS6y87C9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Apply clustering with the selected k"],"metadata":{"id":"NbZZOx8W8BTS"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"xR-GMy467BdZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, you will compress some image using k-means. <br/>\n","Here, you are given image from size 400x600x3. The last parameter is the number of channels. 3 channels means that the image is colored (unlike 1 in, which is grayscale). <br/>\n","Our goal is to reduce the number of colors to 20 and represent (compress) the photo using those 20 colors only. <br/>\n","\n","Motivation: the original image requires 400x600x3x8 bits, while the new image will require only 400x600x5 + 20x24 bits, almost 5 times smaller!<br/>\n","To really do this, we will take the image and treat every pixel as a data point, where each data point is in 3d space (r,g,b). Then, we cluster into 20 centroids, and we assign each pixel to a centroid. This will allow us to represent the image using only 20 colors.\n"],"metadata":{"id":"qbVNVuZo8ObI"}},{"cell_type":"code","source":["#@title helper function\n","\n","import urllib.request\n","\n","def read_image(url):\n","    req = urllib.request.urlopen(url)\n","    arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n","    return cv2.imdecode(arr, -1)"],"metadata":{"cellView":"form","id":"oFBFAZFj-VkY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Complete the missing code"],"metadata":{"id":"7Tfm_EwY9QWy"}},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","import cv2\n","\n","img = read_image('https://sharon.srworkspace.com/ml/datasets/hw4/image.jpg')\n","img_size = img.shape\n","\n","# Reshape it to be 2-dimension\n","X = img.reshape(img_size[0] * img_size[1], img_size[2])\t\t# Turn hxwx3 into (h*w)x3\n","\n","# Run the Kmeans algorithm\n","km = KMeans(n_clusters=20)\n","km.fit(X)\n","\n","'''\n","The km has the following properties:\n","(*) km.labels_ is an array size (pixels, 20), will give each pixel its class from 20 classes (values are between 0-19)\n","(*) km.cluster_centers_ is an array size 20x3, where the ith row represents the color value for the ith label.\n","\tFor example, cluster_centers_[0] = [r,g,b], the first center.\n","'''\n","\n","# Use the centroids to compress the image\n","img_compressed = # Use cluster_centers_ and labels_\n","img_compressed = np.clip(img_compressed.astype('uint8'), 0, 255)\n","\n","# Reshape X_recovered to have the same dimension as the original image 128 * 128 * 3'''\n","img_compressed = img_compressed.reshape(img_size[0], img_size[1], img_size[2])"],"metadata":{"id":"ZjYkvr8J9Bul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Plot the original and the compressed image next to each other'''\n","plt.figure(figsize = (12, 4))\n","\n","plt.subplot(1, 2, 1)\n","plt.imshow(img)\n","plt.title(\"Original Image\")\n","\n","plt.subplot(1, 2, 2)\n","plt.imshow(img_compressed)\n","plt.title(f'Compressed Image with {km.n_clusters} colors')\n","\n","plt.show()"],"metadata":{"id":"rAvDpiqO_S4B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## load smiling dataset\n","\n","In the third question, we will deal with the Smiling-face dataset, which determines if a person is smiling or not. You will try several models and hope to get good results<br/>\n","Your task is: run the following section and make sure your understand what's going on."],"metadata":{"id":"d_iKlHnjsiBj"}},{"cell_type":"markdown","source":["Go to your <a href=\"https://www.kaggle.com/\">Kaggle</a> account and under the settings, generate new API token. <br/>\n","This will give you the json file, which you will upload here."],"metadata":{"id":"Kq3cRhYC6AaA"}},{"cell_type":"code","source":["# The script expects you to upload JSON file to it!\n","\n","! pip install -q kaggle\n","from google.colab import files\n","files.upload()\n","! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json\n","! kaggle datasets list\n","! kaggle datasets download chazzer/smiling-or-not-face-data\n","! unzip -q smiling-or-not-face-data.zip -d data"],"metadata":{"id":"XsC3aagYU8Bg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["import libraries"],"metadata":{"id":"lB0zaNLzslMW"}},{"cell_type":"code","source":["import numpy as np\n","from matplotlib import pyplot as plt\n","import os\n","import cv2\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"V1j2in2zXhT0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["process the images"],"metadata":{"id":"1UTtYOlw9eDa"}},{"cell_type":"code","source":["def proccess_data(folder):\n","\timage_arrays = []\n","\tfor filename in os.listdir(folder):\n","\t\tfile_path = os.path.join(folder, filename)\n","\t\timage = cv2.imread(file_path)\n","\t\tgray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\t\timage_arrays.append(gray_image)\n","\treturn np.array(image_arrays)\n","\n","smile = proccess_data('./data/smile')\n","non_smile = proccess_data('./data/non_smile')\n","\n","dataset = np.vstack((smile,non_smile))\n","dataset = dataset / 255\n","\n","labels = [0] * len(smile) + [1] * len(non_smile)\n","labels = np.array(labels)"],"metadata":{"id":"hvQvtLOAXk8q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["display smiling and non-smiling image."],"metadata":{"id":"1nW9k369dhYT"}},{"cell_type":"code","source":["plt.subplot(121)\n","plt.title(\"Smile\")\n","plt.imshow(smile[0], cmap='gray')\n","\n","plt.subplot(122)\n","plt.title(\"Not smile\")\n","plt.imshow(non_smile[0], cmap='gray')\n","\n","plt.show()"],"metadata":{"id":"QfbqqhCadGRX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["print the smiling and non-smiling data + the united dataset along with labels."],"metadata":{"id":"rEVo_5-k990Z"}},{"cell_type":"code","source":["print(f'smile array size is (images, height, width)={smile.shape}')\n","print(f'non smile array size is (images, height, width)={non_smile.shape}')\n","print()\n","print(f'dataset array size is {dataset.shape}')\n","print(f'labels array size is {labels.shape}')"],"metadata":{"id":"4hCqccgRZUzT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prepear train and test datasets, print their structure. Since you have to deal with 1d features, we flatten the squared image"],"metadata":{"id":"nw3w7aGusyDN"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(dataset, labels, test_size = 0.2, stratify=labels, random_state=42)\n","\n","print(f'train size is {x_train.shape} and labels size is {y_train.shape}')\n","print(f'test size is {x_test.shape} and labels size is {y_test.shape}')\n","print()\n","\n","x_train_flatten = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n","x_test_flatten = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n","\n","print(f'flattened train size is {x_train_flatten.shape} ')\n","print(f'flattened test size is {x_test_flatten.shape}')"],"metadata":{"id":"dhyWZgWXbE4I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 3 - PCA, LDA\n","\n","You will try to classify the smiling faces dataset using feature reduction and KNN (since there are 4096 features!). Than you will compare it to LDA"],"metadata":{"id":"scLdbLSYskMr"}},{"cell_type":"markdown","source":["import libraries"],"metadata":{"id":"u0KjWT27q-kd"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","from sklearn.neighbors import KNeighborsClassifier"],"metadata":{"id":"1tV-D9usq8eo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement PCA to reduce the dimension of the images from 4096=64x64 to 81=9x9. For time effciency, DONT use any loops here.\n","\n","Hint: Implement inverse_transform to recover the original vector from the compressed one. <br/>\n","Hint: When dealing with symmetric matrix, you can call eigh instead of eig function of numpy, its much faster."],"metadata":{"id":"HVPuhAjX-k9f"}},{"cell_type":"code","source":["def PCA_train(data, k):\n","\t# Implement here\n","\t# Download data to k dimensions\n","\n","def PCA_test(test, mu, E):\n","\t# Implement here\n","\n","def recover_PCA(data, mu, E):\n","\t# Implement here"],"metadata":{"id":"edDFB2MLcU4Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Apply the PCA. <br/>\n","Make sure you fit the PCA model only to the training set (but apply it to both training and test sets). <br/>"],"metadata":{"id":"8XGKa-V4ARsq"}},{"cell_type":"code","source":["x_train_new, mu, eig = # Implement here\n","x_test_new = # Implement here"],"metadata":{"id":"_jdtyXCLeGlx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pick another random image and show the result of applying PCA to it, and then try to recover the whole size again."],"metadata":{"id":"4OroYh4NAd6A"}},{"cell_type":"code","source":["plt.subplot(131)\n","plt.title(\"Original Image\")\n","plt.imshow(None, cmap='gray')\n","\n","plt.subplot(132)\n","plt.title(\"Image in lower dimension\")\n","plt.imshow(None, cmap='gray')\n","\n","plt.subplot(133)\n","plt.title(\"Recovered Image\")\n","plt.imshow(None, cmap='gray')\n","\n","plt.show()"],"metadata":{"id":"US1NS8Mdf4ip"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Before training the model, use EIG_CDF, that given eigenvalues, draws a CDF of them like here:<br/><br/>\n","\n","![Picture1.jpg](https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcS3mOZk1x4X3ap9nuMnst5W5pMgOXF8r3Tmx1QcFX9mba_lleuB)\n","\n","As seen in the tutorials, we use them to see how much \"energy\" we preserve from the data. Use this to choose optimal dimension to reduce into, such the preserves 95% of the energy."],"metadata":{"id":"HkI1iur3BDF4"}},{"cell_type":"code","source":["def EIG_CDF(eig_list):\n","\tsorted_eigenvalues = np.sort(eig_list)[::-1]\n","\n","\teigenvalues_cumsum = np.cumsum(sorted_eigenvalues)\n","\n","\teigenvalues_cumsum_normalized = eigenvalues_cumsum / eigenvalues_cumsum[-1]\n","\tamount = # Implement here\n","\n","\tplt.plot(np.arange(1, len(sorted_eigenvalues)+1), eigenvalues_cumsum_normalized)\n","\tplt.xlabel('Principal Component')\n","\tplt.ylabel('Cumulative Proportion of Variance')\n","\tplt.title(f'CDF of Eigenvalues - {amount} eigs preserves 95% of enetry')\n","\tplt.show()\n","\n","# Call to EIG_CDF"],"metadata":{"id":"k-zaqczg_VGr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For the same image as before, show the result of applying PCA to it and recovering.<br/>\n","Is the result better? What is different from 81 dimensions? <br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"O6nbAp0mBuqM"}},{"cell_type":"code","source":["plt.subplot(131)\n","plt.title(\"Original Image\")\n","plt.imshow(None, cmap='gray')\n","\n","plt.subplot(132)\n","plt.title(\"Image in lower dimension\")\n","plt.imshow(None, cmap='gray')\n","\n","plt.subplot(133)\n","plt.title(\"Recovered Image\")\n","plt.imshow(None, cmap='gray')\n","\n","plt.show()"],"metadata":{"id":"2m7DAhXOCerg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, you are ready to train the model. Use KNN, tune the best k using cross_val_score (with sklearn)"],"metadata":{"id":"bqaVkkL6s1rE"}},{"cell_type":"code","source":["# Implement here\n","\n","plt.figure(figsize=(14,5))\n","plt.plot(ks, accs)\n","plt.xlabel('k')\n","plt.xticks(ks)\n","plt.ylabel('avg accuracy')\n","plt.show()"],"metadata":{"id":"lNlJMtGYnh6T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Print the accuracy of your model on the test set."],"metadata":{"id":"rWqFTbTcC8d1"}},{"cell_type":"code","source":["# Implement here\n","print(f'acc on test is {acc}')"],"metadata":{"id":"FvKlB4K3e7sD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Answer the following sum-up questions: <br/>\n","- What pre-proccessing actions were done on the data?\n","- Should we apply Standard Scaler? Why?\n","- Suggest one idea for improvement (rather than LDA)\n","\n","<font color='red'>Write here your answers, with explainations</font>\n"],"metadata":{"id":"CVJW2S-vtDB3"}},{"cell_type":"markdown","source":["For the second part, we will use LDA on the data <b>before</b> PCA. <br/>\n","Use the model of LinearDiscriminantAnalysis from Sklearn, train the data and print the accuracy test using KNN. <br/>\n","Use the best k you found earlier."],"metadata":{"id":"fNIenItVpEYt"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"Q0JrHVlvpbhj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, repeat the same as above, but using the data <b>after</b> PCA and using the same k.\n","- Was the combination of PCA and LDA helpful more than LDA alone?\n","- Comparing LDA alone vs PCA alone (as dimensionality reduction), which one was better to this problem? Justify.\n","\n","<font color='red'>Write your answers here and explain them.</font>"],"metadata":{"id":"hjtGXZ6Kpcu8"}},{"cell_type":"markdown","source":["## Question 4 - Adaboost\n","See attached pdf in moodle assignment!\n","\n"],"metadata":{"id":"DcjNI7QE9j70"}},{"cell_type":"markdown","source":["## Question 5 - Kernel PCA - 10 pts bonus\n","See attached pdf in moodle assignment! <br/>\n","Here you will implement the parts that are relevant for that question"],"metadata":{"id":"E2rCBEixpHac"}},{"cell_type":"code","source":["# Implement here everything you need"],"metadata":{"id":"or0URrvxvBnz"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["Kh7ZuCb6r9Fs","wqPk-EK5tBJT","d_iKlHnjsiBj","scLdbLSYskMr","E2rCBEixpHac"],"mount_file_id":"1Kr1hAcsz3nQe5ce3vdzAXXyH9YeUk5YY","authorship_tag":"ABX9TyM1mEU/byF69UVDYlRU1FVc"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}