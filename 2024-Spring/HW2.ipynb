{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introducrtion to Machine Learning: Assignment #2\n",
        "## Submission date: 18\\06\\2024, 23:59.\n",
        "### Topics:\n",
        "- Perceptron\n",
        "- Logistic Regression\n",
        "- Gradient Descent\n",
        "- SVM\n",
        "- Kernels"
      ],
      "metadata": {
        "id": "aj440z9b98Zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submitted by:\n",
        "\n",
        " **Student 1 Name+ID\n",
        "\n",
        " **Student 2 Name+ID"
      ],
      "metadata": {
        "id": "em4OeZTD9-R2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment Instruction:**\n",
        "\n",
        "· Submissions in pairs only.\n",
        "\n",
        "· Try to keep the code as clean, concise, and short as possible\n",
        "\n",
        "· If you wish to work in your IDE, you can, but you **must**,  insert the script back to the matching cells of the notebook and run the code. <br/>Only the notebook will be submitted in moodle (in `.ipynb` format).\n",
        "\n",
        "· <font color='red'>Please write your answers to question in red</font>.\n",
        "\n",
        "**Important:** All plots, results and outputs should be included in the notebook as the cells' outputs (run all cells and do not clear the output). <br/>\n",
        "\n",
        "**Important:** Your submission must be entirely your own. Any attempts of plagiarism (including ChatGPT) will lead to grade 0 and disciplinary actions.\n"
      ],
      "metadata": {
        "id": "GQ-GUBWN9_JY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 - Logistic regression - Part 1\n",
        "\n",
        "In this section you will build a classifier on a \"toy\" problem - Based on two grades, we determine if student passes the course or not."
      ],
      "metadata": {
        "id": "PJPkQ__X2pKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import libraries and load dataset"
      ],
      "metadata": {
        "id": "KCZXWQdQ4P_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('https://sharon.srworkspace.com/ml/datasets/hw2/exams.csv', header=None)\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "011X9kCz4Prr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets preproccess:\n",
        "- Convert dataset to numpy, make sure classes are {0,1} or {-1,1} (depends of the update rule you choose)\n",
        "- Split to train & test (80-20, starify, random_state=42)\n",
        "- Scale using scaler of your choice\n",
        "\n",
        "<font color='red'>Explain here your scaler choice</font>"
      ],
      "metadata": {
        "id": "U6BK3TZh4SFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Import scaling library\n",
        "\n",
        "# Implement here"
      ],
      "metadata": {
        "id": "3XZQs2952rGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the function Logistic_Regression_via_GD(P,y,lr):\n",
        "-\tInput: an np array ‘X’ of ‘n’ rows and ‘d’ columns, a label vector ‘y’ of ‘n’ entries and learning rate parameter ‘lr’.\n",
        "-\tOutput: The function computes the output vector ‘w’ (and ‘b’) which minimzes the logistic regression cost function on ‘X’ and ‘y’. <br/>\n",
        "\n",
        "The implementation should be fully yours. Don't use library implementation! <br/>\n",
        "It should be done by implementing Gradient descent (with ‘lr’ as the learning rate) to solve logistic regression. <br/>\n",
        "\n",
        "Tip: The gradients may be large, you can use $\\frac{1}{n}\\nabla{L}$ (which is the true empirical loss' gradient)\n",
        "\n"
      ],
      "metadata": {
        "id": "CBR_lRqe8N18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "def Logistic_Regression_via_GD(X, y, lr):\n",
        "  # Implement here"
      ],
      "metadata": {
        "id": "5hgUwWPF8QiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function 'predict' is already implemented for you. It gives a classification for a new sample (x) based on the sign of $w^\\top x + b$. <br/>\n",
        "You may change it if you chose to return {+1,0} instead of {+1,-1}."
      ],
      "metadata": {
        "id": "Zbue-ARwBVMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(w,b,x):\n",
        "    return np.sign(np.dot(w, x) + b)"
      ],
      "metadata": {
        "id": "CqrZl4vfBvGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call ‘Logistic_Regression_via_GD(X,y,lr)’, where ‘X’ and ‘y’ are the training data and the corresponding labels. <br/>\n",
        "Run the model on the test data. With good LR you should get accuracy >= 88\\%"
      ],
      "metadata": {
        "id": "WG7pMuREAWO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_lr = # Add here.\n",
        "w, b = Logistic_Regression_via_GD(X_train, y_train, chosen_lr)\n",
        "\n",
        "preds = predict(w, b, X_test)\n",
        "accuracy = np.sum(preds == y_test) / len(y_test)\n",
        "print(f\"Test accuracy is {accuracy * 100}%\")"
      ],
      "metadata": {
        "id": "6zL-7lp0AGcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize data (on test)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot(data, labels, w, bias):\n",
        "\n",
        "    plt.scatter(data[:,0], data[:,1], c=labels)\n",
        "\n",
        "    a, b, c = w[0], w[1], bias\n",
        "\n",
        "    m = -a / b\n",
        "    b = -c / b\n",
        "\n",
        "    x = np.arange(np.min(data[:,0]), np.max(data[:,0]), 0.1)\n",
        "    y = m * x + b\n",
        "\n",
        "    plt.plot(x, y)\n",
        "\n",
        "    preds = np.sign(np.dot(data, w)+bias)\n",
        "    acc = np.count_nonzero(labels == preds) / len(labels)\n",
        "    plt.title(f\"Accuracy on data is {acc}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot(X_test, y_test, w, b)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S4ITLbDrm6R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 - Logistic regression - Part 2\n",
        "Now we are going to work on a much realistic problem - based on IMDB review, classify whether its positive or negative review."
      ],
      "metadata": {
        "id": "8pyjaie3nmM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import os"
      ],
      "metadata": {
        "id": "dNg15Co9noPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print first review <br/>\n",
        "Tip: Click on file -> mount drive. Since the dataset is heavy (60MB), you don't want to upload it over and over each time. Then, you will be able to open it from your drive storage."
      ],
      "metadata": {
        "id": "Sc74Gf49nqdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('IMDB Dataset.csv')    # you may change the path according to your drive\n",
        "\n",
        "print(f\"The dataset shape is {df.shape}\")\n",
        "\n",
        "# print the first line\n",
        "print(df.iloc[0].review)\n",
        "print(df.iloc[0].sentiment)"
      ],
      "metadata": {
        "id": "L2xhnI-fnpJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the input is a <b>sentence</b>. The proccess of turning words into <u>meaningful</u> continious vectors is called \"tokenization\" (you will learn more in DL and NLP). In the next section you will load the model and proccess the reviews to obtain continious representation for the data"
      ],
      "metadata": {
        "id": "xYhEnrIloC1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model loader function\n",
        "\n",
        "MODEL_PATH = 'wordvec.model'\n",
        "\n",
        "def load_model(data):\n",
        "\tif os.path.exists(MODEL_PATH):\n",
        "\t\treturn gensim.models.Word2Vec.load(MODEL_PATH)\n",
        "\telse:\n",
        "\t\tmodel = Word2Vec(sentences=data, vector_size=100, window=5, min_count=1)\n",
        "\t\tmodel.save(MODEL_PATH)\n",
        "\t\treturn model"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2WGyeAaPoEQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df['review'].tolist()\n",
        "\n",
        "# turn sentences into a list. Each sentence is an array of words\n",
        "tokenized_sentences = [[token for token in row.split()] for row in sentences]\n",
        "\n",
        "# will ~3 minutes. The model is saved to a LOCAL path.\n",
        "model = load_model(tokenized_sentences)     # calls to \"model loader function\" above."
      ],
      "metadata": {
        "id": "tvJYaDa8oTue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = model.wv\n",
        "print(word_vectors['wow'].shape)\n",
        "print(word_vectors['wow'])"
      ],
      "metadata": {
        "id": "NfVInuNfoiWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, each word has now 100 features. <br/>\n",
        "For simplicity, you will represent a sentence by the mean vector of all the words within it. <br/>\n",
        "Your tasks are: <br/>\n",
        "- Make sure classes are {0,1} or {-1,1} (depends of the update rule you choose). You can change that row.\n",
        "- Construct X to be 50000 x 100, where each row represents a sentnce.\n",
        "- Convert it to numpy."
      ],
      "metadata": {
        "id": "0pjTyE3hokLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['sentiment'].replace({'positive': 1, 'negative': -1}).to_numpy()\n",
        "\n",
        "# Implement here the construction of X"
      ],
      "metadata": {
        "id": "UxXzPtOgoj13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into 20\\% test and 80\\% train. After that, split the train into smaller train and 20% validation. Use stratify and random state of 42\\%"
      ],
      "metadata": {
        "id": "kHgtFHz2oq8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "qFDTVI5DotWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, scale the data <br/>\n",
        "Hint: there are many wrong ways to do that scaling (on train validation and test) but only one is correct. Recall What sets are required for validation and for evaluation on test. We discussed this on the second tutorial<br/>\n",
        "<font color='red'>What scaler did you choose and why?</font>"
      ],
      "metadata": {
        "id": "txCEJJNXsnhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "WuTvZAkzsuvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy here your logistic solver from before and adjust its implementation for this problem, if required."
      ],
      "metadata": {
        "id": "3hf0b4b9r2fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "cBaiSwh9r_XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model and evaluate on test set"
      ],
      "metadata": {
        "id": "v8P0myxysRDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_lr = # Add here.\n",
        "w, b = Logistic_Regression_via_GD(X_train, y_train, chosen_lr)\n",
        "\n",
        "preds = predict(w, b, X_test)\n",
        "accuracy = np.sum(preds == y_test) / len(y_test)\n",
        "print(f\"Test accuracy is {accuracy * 100}%\")"
      ],
      "metadata": {
        "id": "OcyB0bvHsT6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer the following two questions: <br/>\n",
        "- Try to justify the accuracy  you got. What should be done differently?\n",
        "- How did you choose your lr? Is it the correct method?\n",
        "\n",
        "<font color='red'>Write your answers here and explain</font>"
      ],
      "metadata": {
        "id": "ZfaKhxK_t0sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 - Perceptron\n",
        "You are given dataset for binary classification in 2D and aim to build the best Perceptron classifier."
      ],
      "metadata": {
        "id": "NKWjqVSG-hZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import libraries"
      ],
      "metadata": {
        "id": "WcTtpyuLRnos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "S9McjsXS-inp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load npy file\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def load_npy_file(url):\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    npy_data = np.load(BytesIO(response.content), allow_pickle=True).item()\n",
        "    return npy_data\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZMbzzvfpxAM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data and make sure labels are appropriate for the perceptron algorithm"
      ],
      "metadata": {
        "id": "KhiHyOHbWCOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = load_npy_file('https://sharon.srworkspace.com/ml/datasets/hw2/perceptron_data.npy')\n",
        "\n",
        "X_train = data_dict['X_train']\n",
        "y_train = data_dict['y_train']\n",
        "X_test = data_dict['X_test']\n",
        "y_test = data_dict['y_test']\n",
        "\n",
        "# Change labels here if required."
      ],
      "metadata": {
        "id": "CnxdtJ9H-uMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the function perceptron(data, labels, lr) which gets the train data along with labels and returns the weight vector learned by perceptron. Moreover, before the return, print the number of iterations it took to converge.\n",
        "\n"
      ],
      "metadata": {
        "id": "tQAuuHqiWjFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron(data, labels, lr = 1):\n",
        "  # Implement here"
      ],
      "metadata": {
        "id": "o7VtxRETAD-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ploting function\n",
        "def plot(data, labels, w, bias):\n",
        "\n",
        "    plt.scatter(data[:,0], data[:,1], c=labels)\n",
        "\n",
        "    a, b, c = w[0], w[1], bias\n",
        "\n",
        "    m = -a / b\n",
        "    b = -c / b\n",
        "\n",
        "    x = np.arange(0.2, 0.8, 0.1)\n",
        "    y = m * x + b\n",
        "\n",
        "    plt.plot(x, y)\n",
        "\n",
        "    preds = np.sign(np.dot(data, w)+bias)\n",
        "    acc = np.count_nonzero(labels == preds) / len(labels)\n",
        "    plt.title(f\"Accuracy on data is {acc}\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s4mWRarOv7Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose LR, train and call the plotting function on the <u>train</u>\n",
        "dataset. It prints the accurcay as the graph's title.\n",
        "1. What is your conclusion about this data?\n",
        "2. How would SVM react to such data?\n",
        "3. What can you tell about the test accuracy?\n",
        "\n",
        "<font color='red'>Write here your answers and explain it.</font>"
      ],
      "metadata": {
        "id": "Ffu9HOcBv6dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_lr = # Add here\n",
        "w = perceptron(X_train, y_train, chosen_lr)\n",
        "w, b = w[:-1], w[-1]\n",
        "plot(X_train, y_train, w, b)"
      ],
      "metadata": {
        "id": "AfQtjdHxCgUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the test accuracy"
      ],
      "metadata": {
        "id": "cnbaTIq9wOZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = np.sign(np.dot(X_test, w) + b)\n",
        "acc = np.count_nonzero(y_test == preds) / len(y_test)\n",
        "plot(X_test, y_test, w, b)"
      ],
      "metadata": {
        "id": "tzt1OEPtwQD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time for some convergence theory! </br>\n",
        "\n",
        "Let ${\\{x_i\\} }_{i=1}^n$ be the training set and $R$ such that $∀i:‖x_i ‖≤R$.<br/>\n",
        "Now, let $γ$ such that $∀i:x_i\\cdot g(x_i)>\\gamma$.<br/>\n",
        "Then, the number of iterations required for convergence is bounded (from above) by $R^2/\\gamma^2$. However, this is true only when using single batch method.\n",
        "\n",
        "Use this theorem to find the the value of upper bound for our problem and use that exact number as upper bound. Report the train accuracy.\n",
        "Did the iterations number improve?<br/>\n",
        "<font color='red'>Write here your answer</font>\n"
      ],
      "metadata": {
        "id": "SDxiA1uuwUJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron_updated(data, labels, lr = 1):\n",
        "  # Implement here"
      ],
      "metadata": {
        "id": "SqCgXdQ7wTrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Report the train accuracy.\n",
        "\n",
        "chosen_lr = # Add here\n",
        "w = perceptron_updated(X_train, y_train, chosen_lr)\n",
        "w, b = w[:-1], w[-1]\n",
        "plot(X_train, y_train, w, b)"
      ],
      "metadata": {
        "id": "vgA590nzwlMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report the test accuracy. Did it improve? What can you conclude about the perceptron algorithm in the linear separable case?"
      ],
      "metadata": {
        "id": "3TVj0tinwpEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = np.sign(np.dot(X_test, w) + b)\n",
        "acc = np.count_nonzero(y_test == preds) / len(y_test)\n",
        "plot(X_test, y_test, w, b)"
      ],
      "metadata": {
        "id": "hcCzpW4cwp38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 - SVM\n",
        "\n",
        "You are given dataset for binary classification in 2D and aim to build the best SVM classifier."
      ],
      "metadata": {
        "id": "JqHofE9ur3xq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import libraries"
      ],
      "metadata": {
        "id": "da1lbeIPNVcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "50PLGF_RNQmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load npy file\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def load_npy_file(url):\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    npy_data = np.load(BytesIO(response.content), allow_pickle=True).item()\n",
        "    return npy_data\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "g3a9t0k76D6g",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load the data"
      ],
      "metadata": {
        "id": "wBn4WFZiNYwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = load_npy_file('https://sharon.srworkspace.com/ml/datasets/hw2/svm_data_2d.npy')\n",
        "\n",
        "# Access the data as needed\n",
        "X_train = data_dict['X_train']\n",
        "y_train = data_dict['y_train']\n",
        "X_val = data_dict['X_val']\n",
        "y_val = data_dict['y_val']"
      ],
      "metadata": {
        "id": "wxGONKCDvHAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the plot of the training data. <br/>\n",
        "What _geometric_ shape could (almost) perfectly separate the data?<br/>"
      ],
      "metadata": {
        "id": "WJSQ7JMWwvA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Generated Train')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vEgrPhvMwwVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hope you answered parabola! <br/>\n",
        "Its equation is:\n",
        "$$Ax^2+By^2+Cx+Dy+e=0$$\n",
        "Based on that equation, construct a mapping function into 4d space, such that the problem will become a linear ($w^t ϕ(x)+e=0$). <br/>\n",
        "After the mapping, learn a linear classifier and print the hyperplane equation.<br/>\n",
        "Note: after getting an output, I would recommend you to plot this equation on desmos, just to \"see it\"."
      ],
      "metadata": {
        "id": "H5blMGXcOC1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_features = # Implement here\n",
        "\n",
        "model = SVC(kernel='linear', C=3)\n",
        "model.fit(new_features, y_train)\n",
        "\n",
        "# Get the hyperplane equation coefficients and intercept\n",
        "coefficients = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "\n",
        "# Print the hyperplane equation\n",
        "equation_parts = []\n",
        "for i in range(len(coefficients)):\n",
        "    equation_parts.append(f\"({coefficients[i]:.3f} * X{i+1})\")\n",
        "equation = \" + \".join(equation_parts) + f\" + ({intercept[0]:.3f})\"\n",
        "\n",
        "print(\"Hyperplane equation:\")\n",
        "print(f\"  {equation}\")"
      ],
      "metadata": {
        "id": "0OyItC8Cv0Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the missing lines to get plots on train and test"
      ],
      "metadata": {
        "id": "bSn-OzSoPNsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = # Implement here\n",
        "train_preds = # Implement here\n",
        "train_acc = # Implement here\n",
        "\n",
        "val_features = # Implement here\n",
        "val_preds = # Implement here\n",
        "val_acc = # Implement here\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(-2, 2.2, 0.1), np.arange(-2, 2.2, 0.1))\n",
        "data = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "new_features = # Implement here the new features construction on 'data'\n",
        "Z = # Implement here the predictions of data into 2 classes, using w, b you found\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Plot the training data on the first subplot\n",
        "axs[0].contourf(xx, yy, Z, alpha=0.8)\n",
        "scatter1 = axs[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr')\n",
        "axs[0].set_xlabel('X')\n",
        "axs[0].set_ylabel('Y')\n",
        "axs[0].set_title(f'Train dataset - {train_acc:.4f} accuracy')\n",
        "\n",
        "# Plot the validation data on the second subplot\n",
        "axs[1].contourf(xx, yy, Z, alpha=0.8)\n",
        "scatter2 = axs[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap='bwr')\n",
        "axs[1].set_xlabel('X')\n",
        "axs[1].set_ylabel('Y')\n",
        "axs[1].set_title(f'Validation dataset - {val_acc:.4f} accuracy')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ah59x2C9OyKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will try also two other kernels:\n",
        "  - kernel='poly', degree=4, C=3.\n",
        "  -\tkernel='rbf', gamma=1.5, C=3.\n",
        "\n",
        "Based on those two models, along with the parabola, which model generalizes the best? <br/>\n",
        "<font color='red'>Write here your answer and explain it</font>"
      ],
      "metadata": {
        "id": "qoIzr9VLPsv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model = # Implement here\n",
        "clf = model.fit(X_train, y_train)\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(-2, 2.2, 0.1), np.arange(-2, 2.2, 0.1))\n",
        "xy = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "P = model.decision_function(xy).reshape(xx.shape)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Plot the training data on the first subplot\n",
        "axs[0].contourf(xx, yy, P, alpha=0.8)\n",
        "scatter1 = axs[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr')\n",
        "axs[0].set_xlabel('X')\n",
        "axs[0].set_ylabel('Y')\n",
        "axs[0].set_title(f'Train dataset - {clf.score(X_train, y_train):.4f} accuracy')\n",
        "\n",
        "# Plot the validation data on the second subplot\n",
        "axs[1].contourf(xx, yy, P, alpha=0.8)\n",
        "scatter2 = axs[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap='bwr')\n",
        "axs[1].set_xlabel('X')\n",
        "axs[1].set_ylabel('Y')\n",
        "axs[1].set_title(f'Validation dataset - {clf.score(X_val, y_val):.4f} accuracy')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IuyEg_B9_4lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model = # Implement here\n",
        "clf = model.fit(X_train, y_train)\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(-2, 2.2, 0.1), np.arange(-2, 2.2, 0.1))\n",
        "xy = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "P = model.decision_function(xy).reshape(xx.shape)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Plot the training data on the first subplot\n",
        "axs[0].contourf(xx, yy, P, alpha=0.8)\n",
        "scatter1 = axs[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr')\n",
        "axs[0].set_xlabel('X')\n",
        "axs[0].set_ylabel('Y')\n",
        "axs[0].set_title(f'Train dataset - {clf.score(X_train, y_train):.4f} accuracy')\n",
        "\n",
        "# Plot the validation data on the second subplot\n",
        "axs[1].contourf(xx, yy, P, alpha=0.8)\n",
        "scatter2 = axs[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap='bwr')\n",
        "axs[1].set_xlabel('X')\n",
        "axs[1].set_ylabel('Y')\n",
        "axs[1].set_title(f'Validation dataset - {clf.score(X_val, y_val):.4f} accuracy')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8sCdYxDyFuii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the validation dataset to tune better hyperparameter for rbf (gamma). Use C=3 as before."
      ],
      "metadata": {
        "id": "qO2xBTlTQJG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gammas = np.arange(0.1, 4.1, 0.1)\n",
        "accs = []\n",
        "\n",
        "for gamma in gammas:\n",
        "  # Implement here\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.plot(gammas, accs, color='red')\n",
        "plt.xlabel('gamma')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('Tuning')\n",
        "plt.xticks(gammas)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QKCt1ZLir4M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "- Without coding, only from the plot, will the accuracy change using the best $\\gamma$ you found? Why?\n",
        "- Why did we use C=3? <br/>\n",
        "- Did you answer to best ml generalizer changed?\n",
        "\n",
        "<font color='red'>Write here your answers and explain it</font>"
      ],
      "metadata": {
        "id": "Fvz0DKtC-mG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 - 10 pts Bonus\n",
        "See attached PDF in Moodle"
      ],
      "metadata": {
        "id": "6uCKMrAFuKNp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PJPkQ__X2pKK",
        "8pyjaie3nmM-",
        "NKWjqVSG-hZG",
        "JqHofE9ur3xq"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}