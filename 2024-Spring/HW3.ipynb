{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introducrtion to Machine Learning: Assignment #3\n",
        "## Submission date: 09\\07\\2024, 23:55.\n",
        "### Topics:\n",
        "- Multiclass Classification\n",
        "- PAC, VCdim\n",
        "- Bias vs Variance\n",
        "- Cross validation\n",
        "- Linear Regression\n",
        "- Decision Trees"
      ],
      "metadata": {
        "id": "ocR88ScnhZjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submitted by:\n",
        "\n",
        " **Student 1 Name+ID\n",
        "\n",
        " **Student 2 Name+ID"
      ],
      "metadata": {
        "id": "k3_EWgIhhddu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment Instruction:**\n",
        "\n",
        "· Submissions in pairs only.\n",
        "\n",
        "· Try to keep the code as clean, concise, and short as possible\n",
        "\n",
        "· If you wish to work in your IDE, you can, but you **must**,  insert the script back to the matching cells of the notebook and run the code. <br/>Only the notebook will be submitted in moodle (in `.ipynb` format).\n",
        "\n",
        "· <font color='red'>Please write your answers to question in red</font>.\n",
        "\n",
        "**Important:** All plots, results and outputs should be included in the notebook as the cells' outputs (run all cells and do not clear the output). <br/>\n",
        "\n",
        "**Important:** Your submission must be entirely your own. Any attempts of plagiarism (including ChatGPT) will lead to grade 0 and disciplinary actions.\n"
      ],
      "metadata": {
        "id": "Z8Atq8JqiCPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 - Multiclass SVM\n",
        "\n",
        "You will implement and compare different multiclass methods in both separable and non separable cases."
      ],
      "metadata": {
        "id": "QjmNAc9voNi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "4H-iYP1J6D3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As seen in tutorials, One vs One is usually a better choice than other methods, even SKlearn uses it! <br/>\n",
        "You are given a partial implementation of OvO classifier and need to complete it. Specifically:<br/>\n",
        "- ```__init__(self, n_classes=2, C=1.0)``` - Init $\\binom{n}{2}$ estimators, one for each pair of classes.\n",
        "\n",
        "- ```fit(self, X, y)``` - given data and labels learns the classifiers using linear SVM. <br/>\n",
        "\n",
        "- ```predict(self, X)``` - as learned in lectures, prediction for sample will be the majority class got most votes from all the classifiers.  Therefore, $\\forall i<j$ predict who is the winner for X in the estimator $(i,j)$ and update the bins accordinly."
      ],
      "metadata": {
        "id": "LjZUMNqT6H0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class OneVsOneClassifier:\n",
        "    def __init__(self, n_classes=2, C=1.0):\n",
        "        # Add code here\n",
        "        self.estimators = defaultdict(None)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for i in range(self.n_classes):\n",
        "            for j in range(i + 1,self.n_classes):\n",
        "                # Add code here\n",
        "                self.estimators[(i,j)].fit(temp_X, temp_y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = np.zeros((self.n_classes, len(X)))\n",
        "        for i in range(self.n_classes):\n",
        "            for j in range(i + 1, self.n_classes):\n",
        "                # Add code here\n",
        "\n",
        "        return np.argmax(scores, axis=0)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        return np.mean(y_pred == y)"
      ],
      "metadata": {
        "id": "zLFSqKd66F85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data"
      ],
      "metadata": {
        "id": "7lAZV_hiGt2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "np.random.seed(2)\n",
        "X, y = make_blobs(n_samples=300,cluster_std=.25, centers=np.array([(-3,1),(0,2),(3,1)]))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)"
      ],
      "metadata": {
        "id": "IfhFT8oYGvvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will test the classifier we just built by plotting confusion matrix and draw the decision boundaries. <br/>\n",
        "Does the value of C matter? <br/>\n",
        "<font color='red'>Write here your answer and explain</font>"
      ],
      "metadata": {
        "id": "lPGtn-5UHkeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_onevsall = OneVsOneClassifier(n_classes=3, C=2)\n",
        "clf_onevsall.fit(X,y)\n",
        "\n",
        "# create a mesh to plot in\n",
        "h = .02  # step size in the mesh\n",
        "x_min, x_max = min(X[:,0])-3,max(X[:,0])+3\n",
        "y_min, y_max = min(X[:,1])-3,max(X[:,1])+3\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "mesh_input = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "Z = clf_onevsall.predict(mesh_input)\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "# Plot also the training points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "\n",
        "metrics.confusion_matrix(y, clf_onevsall.predict(X))"
      ],
      "metadata": {
        "id": "FwEpT6JgGzAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate another data\n",
        "\n"
      ],
      "metadata": {
        "id": "mWtCEnCfII-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2)\n",
        "X, y = make_blobs(n_samples=300,cluster_std=1.25, centers=np.array([(-3,1),(0,2),(3,1)]))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)"
      ],
      "metadata": {
        "id": "ZEFVWH18IL_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the train vs test for different values of C. <br/> Justify the plot, by checking what happens as $C\\rightarrow 0$, $C\\rightarrow \\infty$ and mid value C.<br/>\n",
        "<font color='red'>Write here your answer and explain</font>"
      ],
      "metadata": {
        "id": "FQpZ_c0oIhbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a list of C values\n",
        "C_values = [5e-3, 1e-2, 0.5, 1]\n",
        "\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for C in tqdm(C_values):\n",
        "    # Create a new OneVsOneClassifier instance\n",
        "    clf_onevsall = OneVsOneClassifier(n_classes=3, C=C)\n",
        "\n",
        "    # Fit the classifier on the training data\n",
        "    clf_onevsall.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate the train and test accuracies\n",
        "    train_accuracy = clf_onevsall.score(X_train, y_train)\n",
        "    test_accuracy = clf_onevsall.score(X_test, y_test)\n",
        "\n",
        "    # Append the accuracies to the lists\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "# Plot the train and test accuracies as a function of C\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(C_values, train_accuracies, label=\"Train Accuracy\")\n",
        "plt.plot(C_values, test_accuracies, label=\"Test Accuracy\")\n",
        "plt.xlabel(\"C\")\n",
        "plt.xticks(C_values)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YsU4EVXWIgtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you will experience little bit from softmax. It uses neural networks, but you dont have to understand it. The importand part is the preidction, which is obtained by:\n",
        "```\n",
        "def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return torch.softmax(out, dim=1)\n",
        "```\n",
        "Note that this is exactly that you saw - linear score $s_j$ and then applying softmax.\n",
        "\n",
        "The best results are obtained using 300 epochs (iterations) and in each iteration the batch size is the entire data. <br/>\n",
        "1. What happens as you use 100,600 epcohs compared to 300? why is that?\n",
        "2. Use 300 epochs with batch size 50 vs the entire data. How does the batch size affect the convergence of the problem?\n",
        "\n",
        "<font color='red'>Write here your answers and explain them</font>"
      ],
      "metadata": {
        "id": "xPR_Jf5FKsXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 300\n",
        "batch_size = len(X_train) // 3\n",
        "print(f\"Training size is {X_train} samples and batch size is {batch_size}\")"
      ],
      "metadata": {
        "id": "YD3jntvGK4jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title This will train the model using softmax\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "# Define the simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, output_size)\n",
        "        self.linear1.weight.data.fill_(1)\n",
        "        self.linear1.bias.data.fill_(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        return torch.softmax(out, dim=1)\n",
        "\n",
        "# Set hyperparameters\n",
        "input_size = X_train_tensor.shape[1]\n",
        "output_size = len(torch.unique(y_train_tensor))\n",
        "learning_rate = 0.01\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Instantiate the model and define the optimizer\n",
        "model = SimpleNN(input_size, output_size)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    #if (epoch + 1) % 10 == 0:\n",
        "    #    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the model on test data\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy on test set: {accuracy:.4f}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L62n4wkhK5YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title This will plot the boundaries of the model\n",
        "\n",
        "def predict(data):\n",
        "    outputs = model(torch.tensor(data, dtype=torch.float32))\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    return predicted\n",
        "\n",
        "# create a mesh to plot in\n",
        "h = .02  # step size in the mesh\n",
        "x_min, x_max = min(X[:,0])-3,max(X[:,0])+3\n",
        "y_min, y_max = min(X[:,1])-3,max(X[:,1])+3\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "mesh_input = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "Z = predict(mesh_input)\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "# Plot also the training points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "\n",
        "metrics.confusion_matrix(y, predict(X))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pJ43MKK4K77v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 - Linear Regression\n",
        "You are requested by the Charles Darvin himself to predict the age of abalone from physical measurements.\n",
        "The dataset consists of 7 continious features that he has collected for you."
      ],
      "metadata": {
        "id": "rs2flQjZL-D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Oi-_7SJiMAfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the following:\n",
        "-\tX, 2d matrix from size n x d which represents the training samples.\n",
        "-\ty, array from size n which represents the target value for the corresponding sample.\n",
        "\n",
        "Implement the function Linreg_sol(X,y) which outputs the closed form solution for linear regression on X,y. <br/>\n",
        "Don't use pinv"
      ],
      "metadata": {
        "id": "4zjczKr4MNHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Linreg_sol(X, y):\n",
        "\t# Implement here"
      ],
      "metadata": {
        "id": "RA8-1RrnMOpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data from https://sharon.srworkspace.com/ml/datasets/hw3/abalone.csv"
      ],
      "metadata": {
        "id": "J3qUuz18MR3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "FBnctOlsMVCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your tasks are:\n",
        "- Convert to numpy\n",
        "- Preproccess the data\n",
        "- Find the weight vector and the bias"
      ],
      "metadata": {
        "id": "11RHbDzUM1bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here\n",
        "\n",
        "# Suppose w is the solution\n",
        "print(f'The linear line is {w[1]:.2f}x1+{w[2]:.2f}x2+{w[3]:.2f}x3+{w[4]:.2f}x4+{w[5]:.2f}x5+{w[6]:.2f}x6+{w[7]:.2f}x7+{w[0]:.2f}=0')"
      ],
      "metadata": {
        "id": "R4mxOKwiNOY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, implement 'Linreg_sol_via_GD' which implement gradient descent on the linear regression problem. The stopping criterion should be based only by the iterations number recived as parameter"
      ],
      "metadata": {
        "id": "yaSOp25BOeHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Linreg_sol_via_GD(X, y, lr, iter_number):\n",
        "    w = np.ones(X.shape[1])\n",
        "    # Implement here\n",
        "    return w"
      ],
      "metadata": {
        "id": "ZYAKYLXkOqT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code measures the number of iterations as a function of the error gap between the returned weights and the correct solution. <br/>\n",
        "Conclude the complexity formula required - How many iterations do we need as a function of the error, $\\varepsilon$? Make sure to include the $𝚶$ notation in your answer. <br/>\n",
        "Note: This error is sometimes called 'gap'. <br/>\n",
        "<font color='red'>Write your answer here and explain it</font>"
      ],
      "metadata": {
        "id": "oHM4VX52OyYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_real = w\n",
        "\n",
        "iters = np.arange(1000, 10_000, 100)\n",
        "gaps = []\n",
        "\n",
        "for iter in tqdm(iters):\n",
        "    w = Linreg_sol_via_GD(X, y, lr=0.1, iter)\n",
        "    gaps.append(np.linalg.norm(w_real - w))\n",
        "\n",
        "plt.plot(gaps, iters)\n",
        "plt.ylabel(\"iterations_number\")\n",
        "plt.xlabel(\"error gap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5q_dPBRTOzds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the second part of the task, we will see the features dependence and try to solve it."
      ],
      "metadata": {
        "id": "8t42UEXrQYJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the the value of $\\text{Cov}(X)=\\frac{1}{n-1}(X-\\mu)^\\top (X-\\mu)$? <br/>\n",
        "- What can you imply from this about the features?\n",
        "- Can we imply something on $X^\\top X$? Must its det be 0?\n",
        "\n",
        "Note: In your calculations, make sure the bias is not included as feature. <br/>\n",
        "<font color='red'>Write your answers here and explain it</font>"
      ],
      "metadata": {
        "id": "mibS-tKkNwC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "zjeP8fMxOUyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the solution to ridge regression using GD."
      ],
      "metadata": {
        "id": "2QAw7kjLd6Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Linreg_sol_Ridge(X, y, lamda):\n",
        "    # Implement here"
      ],
      "metadata": {
        "id": "2JrsNgCQeGzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code measures the mse error as function of the regularization hyperparameter lamda. <br/>Explain the origin to the mse' behaviour as function of lamda in this problem. <br/>\n",
        "<font color='red'>Write your answer here and explain it</font>"
      ],
      "metadata": {
        "id": "xQ6qmbqWkq8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mses = []\n",
        "lamdas = np.arange(0.1, 10.1, 0.1)\n",
        "\n",
        "for lamda in lamdas:\n",
        "\n",
        "    w = Linreg_sol_Ridge(X_for_reg, y, lamda)\n",
        "\n",
        "    preds = np.dot(X_for_reg, w)\n",
        "    mses.append(np.mean((preds - y) ** 2))\n",
        "\n",
        "plt.plot(lamdas, mses)\n",
        "plt.ylabel(\"error\")\n",
        "plt.xlabel(\"lamda\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9aJpgYNIkvYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 - Decision tree"
      ],
      "metadata": {
        "id": "loHiZy0lVzT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the wine dataset from hw1, where we classified wine types based on their physical and chemical properties. You will do it, but this time with decision trees!"
      ],
      "metadata": {
        "id": "34peWE4XiSmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "w8LUgNO8V1Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data, print the first rows\n",
        "\n",
        "data = pd.read_csv('https://sharon.srworkspace.com/ml/datasets/hw1/wine.data.csv')\n",
        "data.head(3)"
      ],
      "metadata": {
        "id": "-PZdW625WCTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete this missing implementation of the following functions:\n",
        "- ```calculate_entropy(self, data)``` – given data, compute the entropy, where the distribution is over its labels (target class).\n",
        "- ```calculate_information_gain(self, data, feature)``` – given data and specific feature, compute the information gain given by selecting that feature.\n",
        "\n",
        "Algorithm: The data is continuous, so create 10 thresholds between the min and max values of that feature. For each threshold, split to left tree and right tree and calculate the gain. Choose the threshold which gives the highest gain, along with the gain itself (to later compare between features) <br/>\n",
        "\n",
        "Tip: To split the tree (represented by data df), use filter_data.\n",
        "For example, when calculating the gain of 'skew' with threshold 0.5, you can create the left tree by use ```filter_data(data, 'skew', '0.5', left=True)``` to obtain only those samples."
      ],
      "metadata": {
        "id": "aISL11PniuiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ID3 decision tree class\n",
        "class DecisionTree:\n",
        "\tdef __init__(self):\n",
        "\t\tself.tree = {}\n",
        "\n",
        "\tdef calculate_entropy(self, data):\n",
        "\t\tlabels = data.iloc[:, -1]\n",
        "\t\t# Implement here\n",
        "\n",
        "\tdef calculate_information_gain(self, data, feature):\n",
        "\t\ttotal_entropy = self.calculate_entropy(data)\n",
        "\t\tinformation_gain = total_entropy\n",
        "\n",
        "\t\tvalues = # generate 10 thresholds\n",
        "\t\tbest_treshold = None\n",
        "\t\tbest_gain = 0\n",
        "\t\tfor value in values:\n",
        "\t\t\t# Implement here\n",
        "\n",
        "\t\treturn best_gain, best_treshold\n",
        "\n",
        "\tdef filter_data(self, data, feature, value, left=True):\n",
        "\t\tif left:\n",
        "\t\t\treturn data[data[feature] <= value].drop(feature, axis=1)\n",
        "\t\telse:\n",
        "\t\t\treturn data[data[feature] > value].drop(feature, axis=1)\n",
        "\n",
        "\tdef create_tree(self, data, depth=0):\n",
        "\t\t# Recursive function to create the decision tree\n",
        "\t\tlabels = data.iloc[:, -1]\n",
        "\n",
        "\t\t# Base case: if all labels are the same, return the label\n",
        "\t\tif len(np.unique(labels)) == 1:\n",
        "\t\t\treturn list(labels)[0]\n",
        "\n",
        "\t\tfeatures = data.columns.tolist()[:-1]\n",
        "\t\t# Base case: if there are no features left to split on, return the majority label\n",
        "\t\tif len(features) == 0:\n",
        "\t\t\tunique_labels, label_counts = np.unique(labels, return_counts=True)\n",
        "\t\t\tmajority_label = unique_labels[label_counts.argmax()]\n",
        "\t\t\treturn majority_label\n",
        "\n",
        "\t\tselected_feature = None\n",
        "\t\tbest_gain = 0\n",
        "\t\tbest_treshold = None\n",
        "\n",
        "\t\tfor feature in features:\n",
        "\t\t\tgain, treshold = self.calculate_information_gain(data, feature)\n",
        "\t\t\tif gain >= best_gain:\n",
        "\t\t\t\tselected_feature = feature\n",
        "\t\t\t\tbest_treshold = treshold\n",
        "\t\t\t\tbest_gain = gain\n",
        "\n",
        "\t\t# Create the tree node\n",
        "\t\ttree_node = {}\n",
        "\t\ttree_node[(selected_feature, f\"<={best_treshold}\")] = self.create_tree(self.filter_data(data, selected_feature, best_treshold, left=True), depth+1)\n",
        "\t\ttree_node[(selected_feature, f\">{best_treshold}\")] = self.create_tree(self.filter_data(data, selected_feature, best_treshold, left=False), depth+1)\n",
        "\n",
        "\t\t# check if can unite them.\n",
        "\t\tif not isinstance(tree_node[(selected_feature, f\"<={best_treshold}\")], dict) and \\\n",
        "\t\t\t\tnot isinstance(tree_node[(selected_feature, f\">{best_treshold}\")], dict):\n",
        "\t\t\tif tree_node[(selected_feature, f\"<={best_treshold}\")] == tree_node[(selected_feature, f\">{best_treshold}\")]:\n",
        "\t\t\t\treturn tree_node[(selected_feature, f\"<={best_treshold}\")]\n",
        "\n",
        "\t\treturn tree_node\n",
        "\n",
        "\tdef fit(self, data):\n",
        "\t\tself.tree = self.create_tree(data)\n",
        "\n",
        "\tdef predict(self, X):\n",
        "\t\tX = [row[1] for row in X.iterrows()]\n",
        "\n",
        "\t\t# Predict the labels for new data points\n",
        "\t\tpredictions = []\n",
        "\n",
        "\t\tfor row in X:\n",
        "\t\t\tcurrent_node = self.tree\n",
        "\t\t\twhile isinstance(current_node, dict):\n",
        "\t\t\t\tsplit_condition = next(iter(current_node))\n",
        "\t\t\t\tfeature, value = split_condition\n",
        "\t\t\t\ttreshold = float(value[2:])\n",
        "\t\t\t\tif row[feature] <= treshold:\n",
        "\t\t\t\t\tcurrent_node = current_node[feature, f\"<={treshold}\"]\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tcurrent_node = current_node[feature, f\">{treshold}\"]\n",
        "\t\t\tpredictions.append(current_node)\n",
        "\n",
        "\t\treturn predictions\n",
        "\n",
        "\tdef _plot(self, tree, indent):\n",
        "\t\tdepth = 1\n",
        "\t\tfor key, value in tree.items():\n",
        "\t\t\tif isinstance(value, dict):\n",
        "\t\t\t\tprint(\" \" * indent + str(key) + \":\")\n",
        "\t\t\t\tdepth = max(depth, 1 + self._plot(value, indent + 2))\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(\" \" * indent + str(key) + \": \" + str(value))\n",
        "\t\treturn depth\n",
        "\n",
        "\tdef plot(self):\n",
        "\t\tdepth = self._plot(self.tree, 0)\n",
        "\t\tprint(f'depth is {depth}')\n"
      ],
      "metadata": {
        "id": "-IqUorHMV-tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are ready - define DecisionTree, fit it on the entire data and plot the tree. <br/>\n",
        "The depth of the tree should be 5"
      ],
      "metadata": {
        "id": "IdezpzK9WHwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "HgSriNidWHW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decision tree is pretty large (depth is 5). To solve this, lets modify our DecisionTree: <br/>\n",
        "```\n",
        "def __init__(self, max_depth=np.inf):\n",
        "        self.tree = {}\n",
        "        self.max_depth = max_depth\n",
        "```\n",
        "\n",
        "Modify the rest of the code to stop growing after max_depth. <br/>\n",
        "Hint: When reached to max_depth, should we continue splitting? Which category will be best to be selected?\n"
      ],
      "metadata": {
        "id": "RL_ISCptkHCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using 10 thresholds, compare using max_depth=2 and max_depth=4. Based on the results, which depth is better for our problem in term of ML? <br/>\n",
        "<font color='red'>Write your answer here and explain it</font>"
      ],
      "metadata": {
        "id": "jlbxf36OWuEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "for depth in [2, 4]:\n",
        "  print(f\"------------ max_depth={depth} \"------------\")\n",
        "  # Implement here\n",
        "  print(f'Training accuracy is {acc}')\n",
        "\n",
        "  # Implement here\n",
        "  print(f'Test accuracy is {acc}')\n",
        "  print()"
      ],
      "metadata": {
        "id": "yCV8Ia24WwBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use KFold (as seen in tutorials) for a cross validation search to the best depth for the tree."
      ],
      "metadata": {
        "id": "E14fEnmQwi__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "accs = []\n",
        "\n",
        "for depth in tqdm(range(1,6)):\n",
        "    # Implement here\n",
        "\n",
        "plt.plot(range(1,6), accs)\n",
        "plt.xticks(range(1,6))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SjBmMq6Dwuwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use that depth and print the test score. Is it better generalizer than the first one? What do you conclude about the tuning proccess using validation? (answer to yourselves)."
      ],
      "metadata": {
        "id": "KLHtD2ElyTOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 - PAC, Bias vs Variance\n",
        "See attached pdf in moodle assignment!"
      ],
      "metadata": {
        "id": "VumpPbweXEmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5 - Polynomial regression - 5 pts bonus\n",
        "In this problem you will extend regression to fit nonlinear functions.<br/>\n",
        "The dataset contains one feature (x) and continiuos prediction (y)."
      ],
      "metadata": {
        "id": "2xaWiAsF0pTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "hcvqEURI1G2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load data\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def load_npy_file(url):\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    npy_data = np.load(BytesIO(response.content), allow_pickle=True).item()\n",
        "    return npy_data\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7djVEPUN1lmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = load_npy_file('https://sharon.srworkspace.com/ml/datasets/hw3/linreg_data_2d.npy')\n",
        "\n",
        "x_train = data_dict['x_train']\n",
        "y_train = data_dict['y_train']\n",
        "x_test = data_dict['x_test']\n",
        "y_test = data_dict['y_test']"
      ],
      "metadata": {
        "id": "2mAZ5cnz1qRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the plot of the training data. What do you think was the function generated the data? <br/>\n",
        "<font color='red'>Write your answer here</font>"
      ],
      "metadata": {
        "id": "YKhCv_xc51ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_train, y_train, color='blue', s=2)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Generated Train')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hKxQ_WPg51is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will assume the polynomial regression problem of the following form:\n",
        "$$ y=a_0+a_1x+a_2x^2+...+a_dx^d $$\n",
        "The function ```get_solution``` will find the cofficients, similarly to methods done in simple linear regression. <br/>\n",
        "The function ```calc``` will recieve a new sample and the cofficients found, and will predict the output.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JhkNIlk915eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_solution(X, y, degree=2):\n",
        "    # Implement here\n",
        "\n",
        "def calc(x, coefs):\n",
        "    # Implement here"
      ],
      "metadata": {
        "id": "wNYVnO571y2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the current code with $d=1$ yields a simple regressor.\n",
        "- Which $d$ works best?\n",
        "- According to your previous answer, which $d$ <u>is expected</u> to work the best?\n",
        "- Try to explain what happens when you try $d+1$ and why? Was the solver you implemented enough?\n",
        "\n",
        "<font color='red'>Write your answers here and explain them</font>"
      ],
      "metadata": {
        "id": "u8NvvzDE6mNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xx = np.arange(0, 100, 0.1)\n",
        "yy = []\n",
        "\n",
        "weights = get_solution(x_train, y_train, 1)\n",
        "\n",
        "for samp in xx:\n",
        "  yy.append(calc(samp, weights))\n",
        "\n",
        "plt.scatter(x_train, y_train, color='blue', s=2, label='train')\n",
        "plt.scatter(x_test, y_test, color='red', s=2, label='test')\n",
        "plt.plot(xx, yy, color='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ON7wx4Xp6nDP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QjmNAc9voNi9",
        "rs2flQjZL-D7",
        "loHiZy0lVzT0",
        "2xaWiAsF0pTX"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}