{"cells":[{"cell_type":"markdown","source":["# Introducrtion to Machine Learning: Assignment #3\n","## Submission date: 13\\03\\2024, 23:55.\n","### Topics:\n","- PAC, VCdim\n","- Decision Trees\n","- Cross validation\n","- Random forest\n","- kernel regression"],"metadata":{"id":"ocR88ScnhZjt"}},{"cell_type":"markdown","source":["Submitted by:\n","\n"," **Student 1 Name+ID\n","\n"," **Student 2 Name+ID"],"metadata":{"id":"k3_EWgIhhddu"}},{"cell_type":"markdown","source":["**Assignment Instruction:**\n","\n","· Submissions in pairs only.\n","\n","· The code must be reasonably documented\n","\n","· Try to keep the code as clean, concise, and short as possible\n","\n","· Your submission must be entirely your own. Any attempts of plagiarism (including ChatGPT) will lead to disciplinary actions.\n","\n","· You should save a copy of the notebook to your Drive and answer all the questions inside the notebook, at the designated cells. Only the notebook will be submitted in moodle (in `.ipynb` format).\n","\n","· If you wish to work in your IDE, make a `.py` copy of the notebook, but as you finish insert the script back to the matching cells of the notebook.\n","\n","**Important:** All plots, results and outputs should be included in the notebook as the cells' outputs (run all cells and do not clear the output)."],"metadata":{"id":"Z8Atq8JqiCPl"}},{"cell_type":"markdown","source":["## Question 1 - Decision tree"],"metadata":{"id":"loHiZy0lVzT0"}},{"cell_type":"markdown","source":["You are requested by the central bank of America to construct a classifier, for detection whether a banknote is real or fake. <br/>\n","This will be done by decision tree model. <br/>\n","The dataset consists of 4 features which were obtained from the digitized images of banknotes.<br/>\n","\n","Note that the features are continuous! Therefore, use only one threshold and split according to it.\n"],"metadata":{"id":"34peWE4XiSmH"}},{"cell_type":"markdown","source":["import libraries"],"metadata":{"id":"XJcvL4jyiSAP"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"w8LUgNO8V1Ut"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["load the data, print the first rows"],"metadata":{"id":"EOZUePvCjZjZ"}},{"cell_type":"code","source":["data = pd.read_csv('https://sharon.srworkspace.com/ml/datasets/hw3/banknote_authentication.csv')\n","data.head(3)"],"metadata":{"id":"-PZdW625WCTa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Complete this missing implementation of the following functions:\n","- ```calculate_entropy(self, data)``` – given data, compute the entropy, where the distribution is over its labels (target class).\n","- ```calculate_information_gain(self, data, feature)``` – given data and specific feature, compute the information gain given by selecting that feature.\n","\n","Algorithm: The data is continuous, so create 10 thresholds between the min and max values of that feature. For each threshold, split to left tree and right tree and calculate the gain. Choose the threshold which gives the highest gain, along with the gain itself (to later compare between features) <br/>\n","\n","Tip: To split the tree (represented by data df), use filter_data.\n","For example, when calculating the gain of 'skew' with threshold 0.5, you can create the left tree by use ```filter_data(data, 'skew', '0.5', left=True)``` to obtain only those samples."],"metadata":{"id":"aISL11PniuiG"}},{"cell_type":"code","source":["# Define the ID3 decision tree class\n","class DecisionTree:\n","\tdef __init__(self):\n","\t\tself.tree = {}\n","\n","\tdef calculate_entropy(self, data):\n","\t\tlabels = data.iloc[:, -1]\n","\t\t# Implement here\n","\n","\tdef calculate_information_gain(self, data, feature):\n","\t\ttotal_entropy = self.calculate_entropy(data)\n","\t\tinformation_gain = total_entropy\n","\n","\t\tvalues = # generate 10 thresholds\n","\t\tbest_treshold = None\n","\t\tbest_gain = 0\n","\t\tfor value in values:\n","\t\t\t\t# Implement here\n","\n","\t\treturn best_gain, best_treshold\n","\n","\tdef filter_data(self, data, feature, value, left=True):\n","\t\tif left:\n","\t\t\treturn data[data[feature] <= value].drop(feature, axis=1)\n","\t\telse:\n","\t\t\treturn data[data[feature] > value].drop(feature, axis=1)\n","\n","\tdef create_tree(self, data, depth=0):\n","\t\t# Recursive function to create the decision tree\n","\t\tlabels = data.iloc[:, -1]\n","\n","\t\t# Base case: if all labels are the same, return the label\n","\t\tif len(np.unique(labels)) == 1:\n","\t\t\treturn list(labels)[0]\n","\n","\t\tfeatures = data.columns.tolist()[:-1]\n","\t\t# Base case: if there are no features left to split on, return the majority label\n","\t\tif len(features) == 0:\n","\t\t\tunique_labels, label_counts = np.unique(labels, return_counts=True)\n","\t\t\tmajority_label = unique_labels[label_counts.argmax()]\n","\t\t\treturn majority_label\n","\n","\t\tselected_feature = None\n","\t\tbest_gain = 0\n","\t\tbest_treshold = None\n","\n","\t\tfor feature in features:\n","\t\t\tgain, treshold = self.calculate_information_gain(data, feature)\n","\t\t\tif gain >= best_gain:\n","\t\t\t\tselected_feature = feature\n","\t\t\t\tbest_treshold = treshold\n","\t\t\t\tbest_gain = gain\n","\n","\t\t# Create the tree node\n","\t\ttree_node = {}\n","\t\ttree_node[(selected_feature, f\"<={best_treshold}\")] = self.create_tree(self.filter_data(data, selected_feature, best_treshold, left=True), depth+1)\n","\t\ttree_node[(selected_feature, f\">{best_treshold}\")] = self.create_tree(self.filter_data(data, selected_feature, best_treshold, left=False), depth+1)\n","\n","\t\t# check if can unite them.\n","\t\tif not isinstance(tree_node[(selected_feature, f\"<={best_treshold}\")], dict) and \\\n","\t\t\t\tnot isinstance(tree_node[(selected_feature, f\">{best_treshold}\")], dict):\n","\t\t\tif tree_node[(selected_feature, f\"<={best_treshold}\")] == tree_node[(selected_feature, f\">{best_treshold}\")]:\n","\t\t\t\treturn tree_node[(selected_feature, f\"<={best_treshold}\")]\n","\n","\t\treturn tree_node\n","\n","\tdef fit(self, data):\n","\t\tself.tree = self.create_tree(data)\n","\n","\tdef predict(self, X):\n","\t\tX = [row[1] for row in X.iterrows()]\n","\n","\t\t# Predict the labels for new data points\n","\t\tpredictions = []\n","\n","\t\tfor row in X:\n","\t\t\tcurrent_node = self.tree\n","\t\t\twhile isinstance(current_node, dict):\n","\t\t\t\tsplit_condition = next(iter(current_node))\n","\t\t\t\tfeature, value = split_condition\n","\t\t\t\ttreshold = float(value[2:])\n","\t\t\t\tif row[feature] <= treshold:\n","\t\t\t\t\tcurrent_node = current_node[feature, f\"<={treshold}\"]\n","\t\t\t\telse:\n","\t\t\t\t\tcurrent_node = current_node[feature, f\">{treshold}\"]\n","\t\t\tpredictions.append(current_node)\n","\n","\t\treturn predictions\n","\n","\tdef _plot(self, tree, indent):\n","\t\tdepth = 1\n","\t\tfor key, value in tree.items():\n","\t\t\tif isinstance(value, dict):\n","\t\t\t\tprint(\" \" * indent + str(key) + \":\")\n","\t\t\t\tdepth = max(depth, 1 + self._plot(value, indent + 2))\n","\t\t\telse:\n","\t\t\t\tprint(\" \" * indent + str(key) + \": \" + str(value))\n","\t\treturn depth\n","\n","\tdef plot(self):\n","\t\tdepth = self._plot(self.tree, 0)\n","\t\tprint(f'depth is {depth}')\n"],"metadata":{"id":"-IqUorHMV-tr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You are ready - define DecisionTree, fit it on the entire data and plot the tree. <br/>\n","The depth of the tree should be 4"],"metadata":{"id":"IdezpzK9WHwv"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"HgSriNidWHW9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we will use other method, instead of entropy, $$\\text{gini}(A)=1-\\sum_{i\\in C}p(i)^2$$\n","We will modify our DecisionTree: <br/>\n","```\n","def __init__(self, criterion='entropy'):\n","        self.tree = {} <br/>\n","        self.criterion = criterion\n","```\n","\n","Modify the rest of the code to support criterion, whether is gini or entropy.\n"],"metadata":{"id":"RL_ISCptkHCf"}},{"cell_type":"markdown","source":["Train both criterions for 10 thresholds, print the train and test accuracy."],"metadata":{"id":"jlbxf36OWuEo"}},{"cell_type":"code","source":["train, test = train_test_split(data, test_size=0.2, random_state=42, stratify=data['class'])\n","\n","for criterion in [\"entropy\", \"gini\"]:\n","  print(f\"------------ {criterion} \"------------\")\n","  # Implement here\n","  print(f'Training accuracy is {acc}')\n","\n","  # Implement here\n","  print(f'Test accuracy is {acc}')\n","  print()"],"metadata":{"id":"yCV8Ia24WwBB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train both criterions for 100 thresholds, print the tain and test accuracy.<br/> Which criterion is considered better in ML terms? <br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"OZCQ4jl0mfOi"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"_9iTcwtL2cZ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 2 - Random forest"],"metadata":{"id":"3hAgcBEZ6unF"}},{"cell_type":"markdown","source":["Ensemble learning is a general technique to combat overfitting, by combining the predictions of many varied models into a single prediction based on majority vote or their average.<br/>\n","\n","You will extend the tree from question 2 into Random Forest"],"metadata":{"id":"itnGw5A3nGQ0"}},{"cell_type":"markdown","source":["import libraries"],"metadata":{"id":"cEs-RMATnAlN"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","import sys\n","from tqdm import tqdm\n","from sklearn.model_selection import KFold"],"metadata":{"id":"QmSlRnw-6v1a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split into train and test"],"metadata":{"id":"Lz1fcBb1s1z6"}},{"cell_type":"code","source":["data = pd.read_csv('https://sharon.srworkspace.com/ml/datasets/hw3/banknote_authentication.csv')\n","train, test = train_test_split(data, test_size=0.2, stratify=data['class'], random_state=42)"],"metadata":{"id":"9H_RiIZ_7_3v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Complete the missing implementation of the ```fit``` function. <br/>\n","Algorithm: Learn n_estimators trees. Each tree draws data, using sample_data and then selects features randomly, by select_features function. Finally, fit each tree to it's data."],"metadata":{"id":"YA69AoH9nP9k"}},{"cell_type":"code","source":["# Define the ID3 decision tree class\n","class RandomForest:\n","\tdef __init__(self, n_estimators=3, method='simple', criterion='entropy'):\n","\t\tself.forest = []\n","\t\tself.criterion = criterion\n","\t\tself.n_estimators = n_estimators\n","\t\tself.method = method\n","\n","\tdef select_features(self, data):\n","\t\tnp.random.seed(40+len(self.forest))\n","\n","\t\tif self.method == 'sqrt':\n","\t\t\tm = int(np.sqrt(len(data.columns)-1))\n","\t\telif self.method == 'log':\n","\t\t\tm = int(np.log2(len(data.columns)-1))\n","\t\telse:\n","\t\t\tm = np.random.randint(0, len(data.columns))\n","\n","\t\tincidies = np.random.choice(np.arange(0, len(data.columns)-1), size=m, replace=False)\n","\t\tfeatures = list(data.columns[incidies])\n","\t\treturn data[features + ['class']]\n","\n","\tdef sample_data(self, data):\n","\t\t# This method samples len(data) with repitition from data.\n","\t\t# You can use numpy to select random incidies.\n","\n","\tdef fit(self, data):\n","\t\tself.forest = []\n","\t\tfor _ in range(self.n_estimators):\n","\t\t\tsamp_data = data.iloc[self.sample_data(data)]\n","\t\t\t# Implement here\n","\n","\tdef _predict(self, X):\n","\t\t# Predict the labels for new data points\n","\t\tpredictions = []\n","\n","\t\tpreds = [tree.predict(X) for tree in self.forest]\n","\t\tpreds = list(zip(*preds))\n","\t\tpredictions = [Counter(est).most_common(1)[0][0] for est in preds]\n","\n","\t\treturn predictions\n","\n","\tdef score(self, X):\n","\t\tpred = self._predict(X)\n","\t\treturn (pred == X.iloc[:,-1]).sum() / len(X)"],"metadata":{"id":"9e_yk12e6wko"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare Random Forest with 3 trees using entropy vs gini, on both train and test set. Each tree should use 100 thresholds.\n","<br/> Which criterion is considered better in ML terms? <br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"tQEZsoqXtLBN"}},{"cell_type":"code","source":["dict1 = {'entropy': [], 'gini': []}\n","\n","criterions = ['entropy', 'gini']\n","for crt in criterions:\n","  forest = RandomForest(n_estimators=3, method='simple', criterion=crt)\n","  forest.fit(train)\n","\n","  acc = forest.score(train)\n","  dict1[crt].append(acc)\n","\n","  acc = forest.score(test)\n","  dict1[crt].append(acc)\n","\n","print('using 3 estimators')\n","df = pd.DataFrame(dict1, columns=criterions, index=['train', 'test'])\n","print(df)"],"metadata":{"id":"Er_8x3Ls8CWP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You think that Sharon cheated and maybe by using another number of estimators you can get better accuracy. You will use cross validation.<br/>\n","Complete the missing code in the KFold function.<br/>\n","Tip: We saw a similar example code in the tutorials."],"metadata":{"id":"HrDmJRy_tc8j"}},{"cell_type":"code","source":["def KFold2(data, model, cv=5):\n","  kf = KFold(n_splits=cv)\n","  scores = []\n","\n","  for train_index, test_index in kf.split(data):\n","    # Implement here\n","\n","  return np.mean(scores)"],"metadata":{"id":"7WecNNzA9mUE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prepear graph to compare both criterions along different number of estimators.<br/>\n","Tip: run it for one iteration first, to ensure your code works"],"metadata":{"id":"aJy9GATkIy1O"}},{"cell_type":"code","source":["correct_entropy = []\n","correct_gini = []\n","\n","for i in tqdm(range(3,13,2)):\n","\tforest = RandomForest(n_estimators=i, method='simple', criterion='gini')\n","\tcorrect_gini.append(KFold2(data=train, model=forest, cv=5))\n","\tforest = RandomForest(n_estimators=i, method='simple', criterion='entropy')\n","\tcorrect_entropy.append(KFold2(data=train, model=forest, cv=5))\n","\n","plt.plot(range(3,13,2), np.array(correct_entropy), label='entropy')\n","plt.plot(range(3,13,2), np.array(correct_gini), label='gini')\n","\n","plt.legend(loc='upper left')\n","plt.xlabel('trees num')\n","plt.ylabel('avg accuracy')\n","plt.show()"],"metadata":{"id":"qwpQwtdh9n4e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use the best number of estimators you found, retrain the model and print the accuracies on train and test. <br/>\n","Now, which model is better generalizer? <br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"NbalSjnfuoMA"}},{"cell_type":"code","source":["dict1 = {'entropy': [], 'gini': []}\n","\n","criterions = ['entropy', 'gini']\n","for crt in criterions:\n","  forest = RandomForest(n_estimators=best_n, method='simple', criterion=crt)\n","  forest.fit(train)\n","\n","  acc = forest.score(train)\n","  dict1[crt].append(acc)\n","\n","  acc = forest.score(test)\n","  dict1[crt].append(acc)\n","\n","print(f'using {best_n} estimators')\n","df = pd.DataFrame(dict1, columns=criterions, index=['train', 'test'])\n","print(df)"],"metadata":{"id":"8cZpTzkIBoFq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Answer the following questions:\n","- Compared to decision trees, what is the conclusion about the Random Forest model, in terms of generalization, overfitting and correlation?\n","- In our task, we used only method='simple', but not 'square' nor 'log'. Without running, would they work? Why?<br/>\n","\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"Qr4txG3BvJWm"}},{"cell_type":"markdown","source":["## Question 3 - Linear kernelization"],"metadata":{"id":"D6EK6Vxivmng"}},{"cell_type":"markdown","source":["You are given dataset for regression problem, with one feature (x) and prediction (y) and aim to build the best regressor."],"metadata":{"id":"HOhAImvmvquP"}},{"cell_type":"markdown","source":["import libraries"],"metadata":{"id":"XsZKAQpfvzSy"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","from mpl_toolkits.mplot3d import Axes3D"],"metadata":{"id":"OktEMhrFvz52"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["load the data"],"metadata":{"id":"mQqpHuSAv7jQ"}},{"cell_type":"code","source":["import requests\n","from io import BytesIO\n","\n","def load_npy_file(url):\n","  response = requests.get(url)\n","  if response.status_code == 200:\n","    npy_data = np.load(BytesIO(response.content), allow_pickle=True).item()\n","    return npy_data\n","  else:\n","    return None"],"metadata":{"id":"fkTA3Z4B8RI_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the dictionary\n","data_dict = load_npy_file('https://sharon.srworkspace.com/ml/datasets/hw3/linreg_data_2d.npy')\n","\n","# Access the data as needed\n","x_train = data_dict['x_train']\n","y_train = data_dict['y_train']\n","x_test = data_dict['x_test']\n","y_test = data_dict['y_test']"],"metadata":{"id":"I1kacfdVOazn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Look at the plot of the training data. <br/>"],"metadata":{"id":"-BQvjVJBv-XG"}},{"cell_type":"code","source":["plt.scatter(x_train, y_train, color='blue', s=2)\n","plt.xlabel('X')\n","plt.ylabel('Y')\n","plt.title('Generated Train')\n","plt.show()"],"metadata":{"id":"ElyUJojEwCa7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You will use kernelized ridge regression (as learned in tutorial 6) and assume b=0. <br/>\n","Implement the following functions:\n","-\t```kernel(xi, xj, sigma)```: returns the gaussian kernel value $K(x_i,x_j)$.\n","-\t```prepear_kernel_matrix(train)```: using the training, return the Kernel matrix, sized n x n.\n","-\t```get_alphas(kernel, target)```: returns the alphas, as learned.\n","-\t```predict(alphas, train, test)```: predict the regressor for one test sample.\n","\n","For efficiency, make sure ```kernel```,```predict``` won't contain any loops, and ```prepear_kernel``` will only contain one loop"],"metadata":{"id":"_22-1IsvwPYN"}},{"cell_type":"code","source":["def kernel(xi, xj, sigma):\n","  # Implement here\n","\n","def prepear_kernel_matrix(train, sigma):\n","  K = np.zeros((len(train), len(train)))\n","  # Implement here\n","\n","def get_alphas(kernel, target, lamda=0.01):\n","  # Implement here\n","\n","def predict(alphas, train, test, sigma):\n","  # Implement here"],"metadata":{"id":"GPe3CkKPTHpT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use $\\sigma=4$ and plot in black the regressor."],"metadata":{"id":"qwfgxc-FwqR8"}},{"cell_type":"code","source":["xx = np.arange(0, 100, 0.1).reshape((1000,1))\n","yy = []\n","train_kernel = prepear_kernel_matrix(x_train, sigma=4)\n","alphas = get_alphas(train_kernel, y_train)\n","\n","for samp in xx:\n","  yy.append(predict(alphas, x_train, samp, sigma=4))\n","\n","plt.scatter(x_train, y_train, color='blue', s=2, label='train')\n","plt.scatter(x_test, y_test, color='red', s=2, label='test')\n","plt.plot(xx, yy, color='black')\n","plt.show()"],"metadata":{"id":"kPdS472JVSkI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Print MSE of train and test."],"metadata":{"id":"AHM-dbIBpeBi"}},{"cell_type":"code","source":["mse = 0\n","for idx, samp in enumerate(x_train):\n","  mse += (predict(alphas, x_train, samp, sigma=4) - y_train[idx]) ** 2\n","mse = mse / len(x_train)\n","print(f'train mse is {mse}')\n","\n","mse = 0\n","for idx, samp in enumerate(x_test):\n","  mse += (predict(alphas, x_train, samp, sigma=4) - y_test[idx]) ** 2\n","mse = mse / len(x_test)\n","print(f'test mse is {mse}')"],"metadata":{"id":"ok1vyjsnpgft"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create validation set with 20 samples, using train_test_split and random search to tune better $λ,\\sigma$. <br/>"],"metadata":{"id":"GAxzUOamxeDV"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"iJkWjAWomB8z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train the model again on all the training, on the best tuned hyperparamters. Don't forget to re-generate the alphas<br/>\n","Print the new train and test MSE."],"metadata":{"id":"CGS2ym1Dri20"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"IW2VQEU9riiJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot new vs old regressor. Is the result better? (answer for yorselves)"],"metadata":{"id":"JbUgLrYkykae"}},{"cell_type":"code","source":["xx = np.arange(0, 100, 0.1).reshape((1000,1))\n","yy2 = []\n","\n","for samp in xx:\n","  yy2.append(predict(alphas, x_train, samp, sigma=best_sigma))\n","\n","plt.scatter(x_train, y_train, color='blue', s=2, label='train')\n","plt.scatter(x_test, y_test, color='red', s=2, label='test')\n","plt.plot(xx, yy, color='black', label='origin plot')\n","plt.plot(xx, yy2, color='green', label='tuned plot')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"zXfUxzhGbztN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 4 - PAC\n","See attached pdf in moodle assignment!"],"metadata":{"id":"VumpPbweXEmp"}},{"cell_type":"markdown","source":["## Question 5 - 3 points bonus"],"metadata":{"id":"Qp6MpK5Ivj8y"}},{"cell_type":"markdown","source":["Recall the data you've worked with in hw1. Now we are going to look at it again but apply ridge regression"],"metadata":{"id":"WeAwl1Ipvtfh"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","data = np.array([[0.4, 3.4], [0.95, 5.8], [0.16, 2.9], [0.7, 3.6], [0.59, 3.27], [0.11, 1.89], [0.05, 4.5], [0.6, 5.0]])"],"metadata":{"id":"ZuUauQv5vjjw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["solve and plot the regular line. Bad line, bad!"],"metadata":{"id":"Ds3fx2SbvwZi"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","\n","reg = LinearRegression().fit(data[:,:1], data[:,1])\n","\n","print(\"MSE IS \", reg.score(data[:,:1], data[:,1]))\n","\n","x = np.arange(-0.01, 1, 0.01)\n","y = reg.coef_ * x + reg.intercept_\n","plt.plot(x,y)\n","\n","plt.scatter(data[:,0], data[:,1], color='blue', label='Data')\n","plt.show()"],"metadata":{"id":"3rNoTulUvy6q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, do the same with ridge. Better?"],"metadata":{"id":"Sxex6NgDv6v4"}},{"cell_type":"code","source":["from sklearn.linear_model import Ridge\n","\n","reg = Ridge(alpha=1).fit(data[:,:1], data[:,1])\n","\n","print(\"MSE IS \", reg.score(data[:,:1], data[:,1]))\n","\n","x = np.arange(-0.01, 1, 0.01)\n","y = reg.coef_ * x + reg.intercept_\n","plt.plot(x,y)\n","\n","plt.scatter(data[:,0], data[:,1], color='blue', label='Data')\n","plt.show()"],"metadata":{"id":"FShXMzY1v6fN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Robust linear regression!"],"metadata":{"id":"ykZxBR3Kv9hp"}},{"cell_type":"code","source":["from sklearn.linear_model import RANSACRegressor\n","\n","reg = RANSACRegressor(random_state=0).fit(data[:,:1], data[:,1])\n","\n","print(\"MSE IS \", reg.score(data[:,:1], data[:,1]))\n","\n","x = np.arange(-0.01, 1, 0.01)\n","y = reg.predict(x[:,np.newaxis])\n","plt.plot(x,y)\n","\n","plt.scatter(data[:,0], data[:,1], color='blue', label='Data')\n","plt.show()"],"metadata":{"id":"IXoV2Elyv_F4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Actually, robust linear regression has the following objective:\n","$$ w,b=\\text{arg}\\min_{w,b} ||\\bar{X}w-b||_2^2+x^\\top P x$$\n","$U$ is random noise, $\\mathbb{E}[U]=0$ and $P=U^\\top U$. Moreover, $\\bar{X}=X+U$.\n","where $X$ is our original data. We already saw that regular linear regression works under the assumption of gaussian noise, but what happens when $P=\\delta I$? <br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"8fxeiUanwHaK"}}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["loHiZy0lVzT0","3hAgcBEZ6unF","D6EK6Vxivmng","Qp6MpK5Ivj8y"],"mount_file_id":"1Kr1hAcsz3nQe5ce3vdzAXXyH9YeUk5YY","authorship_tag":"ABX9TyPNVgOIZPIDhCvb7HjK94Nf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}