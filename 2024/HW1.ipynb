{"cells":[{"cell_type":"markdown","source":["# Introducrtion to Machine Learning: Assignment #1\n","## Submission date: 31\\1\\2024, 23:59.\n","### Topics:\n","- Linear regression\n","- Gaussian Bayes\n","- Naïve bayes\n","- KNN"],"metadata":{"id":"WxQj_LVp7onx"}},{"cell_type":"markdown","source":["Submitted by:\n","\n"," **Student 1 Name+ID\n","\n"," **Student 2 Name+ID"],"metadata":{"id":"Mih4qIYq7so3"}},{"cell_type":"markdown","source":["**Assignment Instruction:**\n","\n","· Submissions in pairs only.\n","\n","· The code must be reasonably documented\n","\n","· Try to keep the code as clean, concise, and short as possible\n","\n","· Your submission must be entirely your own. Any attempts of plagiarism (including ChatGPT) will lead to disciplinary actions.\n","\n","· You should save a copy of the notebook to your Drive and answer all the questions inside the notebook, at the designated cells. Only the notebook will be submitted in moodle (in `.ipynb` format).\n","\n","· If you wish to work in your IDE, make a `.py` copy of the notebook, but as you finish insert the script back to the matching cells of the notebook.\n","\n","**Important:** All plots, results and outputs should be included in the notebook as the cells' outputs (run all cells and do not clear the output).\n"],"metadata":{"id":"D3nU_S097vG5"}},{"cell_type":"markdown","source":["## Question 1 - Linear regression"],"metadata":{"id":"GeVCGEMFSTD5"}},{"cell_type":"markdown","source":["You will implement simple linear regression alone! <br/> The dataset consists of few 1-feature samples $\\{(x_i,y_i )\\}_{i=1}^∞$ where $y_i$ is the prediction of the $x_i$ sample. <br/>\n","We will only try to fit the given data, <u>without validation or test</u>.<br/>\n","We define the following:\n","-\tX, 2d matrix from size n x d which represents the training samples.\n","-\ty, array from size n which represents the target value for the corresponding sample.\n"],"metadata":{"id":"R500wzm78Egr"}},{"cell_type":"markdown","source":["import libarires"],"metadata":{"id":"peqJrR37SVQa"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"jLPAazuoSUzI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the function Linreg_sol(X,y) which outputs the closed form solution for linear regression on X,y. <br/>Assume the data is already zero-centered"],"metadata":{"id":"wXZwduofSpNo"}},{"cell_type":"code","source":["def Linreg_sol(X, y):\n","\t# Implement here"],"metadata":{"id":"r9jo9K-fSnXV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["first visualization"],"metadata":{"id":"x6A35INtXDwN"}},{"cell_type":"code","source":["data = np.array([[0.4, 3.4], [0.95, 5.8], [0.16, 2.9], [0.7, 3.6], [0.59, 3.27], [0.11, 1.89], [0.05, 4.5]])\n","plt.scatter(data[:,0], data[:,1], color='blue', label='Data')\n","plt.show()"],"metadata":{"id":"b7b0ahPkXDVx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split the data to A,b.<br/>\n","Since the data is non-zero centered, center it to mean zero (since we discussed only y=wx). Then, run the solution."],"metadata":{"id":"uUcARxezS3p9"}},{"cell_type":"code","source":["X, y = # Implement here\n","w = Linreg_sol(X, y)\n","\n","# Restore the original line. if y'=wx' (after removing bias) than y-u_y = w(x-u_x), isolate y.\n","print(f'The linear line is y={w:.2f}*(x-{mean[0]:.2f})+{mean[1]:.2f}')"],"metadata":{"id":"kMb5ULKjTE0q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the line solution <br/>\n","Does the line really fits the data? <br/>\n","<font color='red'>Write here your answer and explain why</font>"],"metadata":{"id":"qYqMQP_JcTb-"}},{"cell_type":"code","source":["x = np.arange(-0.01, 1, 0.01)\n","y = w * (x - mean[0]) + mean[1]\n","plt.plot(x,y)\n","\n","plt.scatter(data[:,0], data[:,1], color='blue', label='Data')\n","plt.show()"],"metadata":{"id":"J7eSeWicVvCH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we learned, try scaling using standardization, repeat the above process.<br/>DON'T use sklearn"],"metadata":{"id":"IcAr2ATpctbj"}},{"cell_type":"code","source":["mean = # Implement here\n","std = # Implement here\n","w = # Implement here\n","\n","# Restore the original line. if y'=wx' (after standardization) than (y-u_y)/std_y = w(x-u_x)/std_x, isolate y.\n","print(f'The linear line is y=({w:.2f}*((x-{mean[0]:.2f})/{std[0]:.2f})*{std[1]:.2f}+{mean[1]:.2f})')"],"metadata":{"id":"MtxhDjtWcuY6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the line solution <br/>\n","Is the result better? <br/>\n","Hint: compare both lines obtained. Why do you think this happened? <br/>\n","<font color='red'>Write here your answer and explain why</font>\n"],"metadata":{"id":"xKZNJ6_sdUVS"}},{"cell_type":"code","source":["x = np.arange(-0.01, 1, 0.01)\n","y = w * (x - mean[0]) * std[1] / std[0] + mean[1]\n","plt.plot(x,y)\n","\n","plt.scatter(data[:,0], data[:,1], color='blue', label='Data')\n","plt.show()"],"metadata":{"id":"5wukppWkdVZo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We Say that a point as outlier if it is located further than one standard deviation above or below the best-fit line. <br/>\n","Find and print the outliers from the (original) dataset\n"],"metadata":{"id":"DX6sHsQAgM2g"}},{"cell_type":"code","source":["# Implement here and print the point that is outlier."],"metadata":{"id":"XwK7E3JPgaSY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the linear regression again, but remove the outliers <br/>\n","Is the result better? <br/>\n","<font color='red'>Write here your answer and explain</font>"],"metadata":{"id":"aGqDS7HKiDTo"}},{"cell_type":"code","source":["data = np.array([[0.4, 3.4], [0.95, 5.8], [0.16, 2.9], [0.7, 3.6], [0.59, 3.27], [0.11, 1.89], [0.05, 4.5]])\n","# remove outliers from data\n","\n","w = # Implement here\n","\n","x = np.arange(-0.01, 1, 0.01)\n","y = w * (x - mean[0]) * std[1] / std[0] + mean[1]\n","plt.plot(x,y)\n","\n","plt.scatter(data[:,0], data[:,1], color='blue', label='Data')\n","plt.show()"],"metadata":{"id":"WapbypFwh45u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 2 - Gaussian bayes\n"],"metadata":{"id":"prp9oMrgRud7"}},{"cell_type":"markdown","source":["You are given data of wine bottles and want to learn the type of wine which it belongs to. The dataset consists of 13 continuous features and 3 classes of wine. <br/>\n","Since the data is continuous, you will implement Gaussian bayes and compare to Gaussian naïve bayes."],"metadata":{"id":"GjvP4MCbHTw2"}},{"cell_type":"markdown","source":["import libarires"],"metadata":{"id":"OoYNLo6XIGlP"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"metadata":{"id":"rGqprtZoIKoW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the wine dataset from https://sharon.srworkspace.com/ml/datasets/hw1/wine.data.csv"],"metadata":{"id":"uc_ziqtWIO6M"}},{"cell_type":"code","source":["# Implement here\n","\n","print(df.shape)\n","df.head(5)"],"metadata":{"id":"Amp-_NESRuBy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check if there is even potential for gaussian assumption to work here, by plotting the density of the features, using plotting for data frames. <br/>\n","Will gaussian bayes work here or not? <br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"RiVF3FCcJFSv"}},{"cell_type":"code","source":["df.plot(kind='density', subplots=True, layout=(4,4), figsize=(18, 15), sharex=False)\n","plt.show()"],"metadata":{"id":"KhjjTrbUR4JR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convert the data to numpy and split the data to 80% training and 20% test with random state of 25. <br/>Note that the data frame currently includes the labels as well."],"metadata":{"id":"UKWn5AX9U1LC"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"dm90izRgU66b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the functions below. <br/>Both get test point x and return the predicted class, but the naïve bayes assumes that the features are independent.<br/>\n","Hint for efficient implementation: you don't need more than one loop, use numpy!"],"metadata":{"id":"fQ3PuGGEXVv8"}},{"cell_type":"code","source":["def classify_point_gaussian_bayes(x):\n","  # Implement here\n","\n","def classify_point_gaussian_naive_bayes(x):\n","  # Implement here"],"metadata":{"id":"Oj5jZiTWV7k6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Look at the test accuracies for both methods. Explain the differences origin. <br/>\n","Hint: Use correlation matrix visualization for data frames.  \n","<font color='red'>Write here your answer and explain it</font>\n","\n","<br/> Reminder: success rate is the precentage of correctly classified data within the number of all data in the test set."],"metadata":{"id":"Qo7NaPefNLIf"}},{"cell_type":"code","source":["res = []\n","for idx, test_point in enumerate(x_test):\n","  res.append(classify_point_gaussian_bayes(test_point) == y_test[idx])\n","print(f'Test accuracy for gaussian bayes is {res.count(True)/len(res)}')\n","\n","res = []\n","for idx, test_point in enumerate(x_test):\n","  res.append(classify_point_gaussian_naive_bayes(test_point) == y_test[idx])\n","print(f'Test accuracy for gaussian naive bayes is {res.count(True)/len(res)}')"],"metadata":{"id":"dXLQ4bS6NAP2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, scale the data using StandardScaler.<br/>\n","Make sure that you are scaling the test according to the training (as learned in class)\n"],"metadata":{"id":"qCLoHc1aNaj1"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"Q-3pcQF8NeP0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Look at the test accuracies now. What can you conclude about the effect  of scaling? Is it good/bad? <br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"jPw2MqXvNoyh"}},{"cell_type":"code","source":["res = []\n","for idx, test_point in enumerate(x_test_scaled):\n","  res.append(classify_point_gaussian_bayes(test_point) == y_test[idx])\n","print(f'Test accuracy for gaussian bayes is {res.count(True)/len(res)}')\n","\n","res = []\n","for idx, test_point in enumerate(x_test_scaled):\n","  res.append(classify_point_gaussian_naive_bayes(test_point) == y_test[idx])\n","print(f'Test accuracy for gaussian naive bayes is {res.count(True)/len(res)}')"],"metadata":{"id":"Pb3K44ulNpGV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the boundaries plotting for (scaled) train and test with gaussian bayes.<br/>It will show the decision boundaries as saw in the tutorials."],"metadata":{"id":"FNr_1jPXN4J2"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","from tqdm import tqdm\n","\n","# Reduce the dimensionality of the data to 2 using PCA\n","pca = PCA(n_components=2)\n","X_reduced = pca.fit_transform(x_train_scaled)\n","\n","# Create a grid of points for visualization in the reduced 2D space\n","x_min, x_max = X_reduced[:, 0].min() - 1, X_reduced[:, 0].max() + 1\n","y_min, y_max = X_reduced[:, 1].min() - 1, X_reduced[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n","\n","# Use the GNB model to predict class labels for the grid points in the original 13D space\n","grid_points = pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()])\n","print(grid_points.shape)\n","Z = np.zeros(len(grid_points))\n","for idx, test_point in tqdm(enumerate(grid_points)):\n","  Z[idx] = classify_point_gaussian_bayes(test_point)\n","Z = Z.reshape(xx.shape)\n","\n","# Plot the decision boundaries and the data points in the reduced 2D space\n","plt.contourf(xx, yy, Z, alpha=0.8)\n","scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_train, cmap=plt.cm.RdYlBu)\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","\n","# Add a legend\n","handles, labels = scatter.legend_elements()\n","plt.legend(handles, labels, title='Classes')\n","\n","plt.title('Train Decision Boundaries of Gaussian Bayes (2D)')\n","plt.show()"],"metadata":{"id":"dCjYD-nsoqRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","from tqdm import tqdm\n","\n","# Reduce the dimensionality of the data to 2 using PCA\n","X_reduced = pca.transform(x_test_scaled)\n","\n","# Create a grid of points for visualization in the reduced 2D space\n","x_min, x_max = X_reduced[:, 0].min() - 1, X_reduced[:, 0].max() + 1\n","y_min, y_max = X_reduced[:, 1].min() - 1, X_reduced[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n","\n","# Use the GNB model to predict class labels for the grid points in the original 13D space\n","grid_points = pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()])\n","print(grid_points.shape)\n","Z = np.zeros(len(grid_points))\n","for idx, test_point in tqdm(enumerate(grid_points)):\n","  Z[idx] = classify_point_gaussian_bayes(test_point)\n","Z = Z.reshape(xx.shape)\n","\n","# Plot the decision boundaries and the data points in the reduced 2D space\n","plt.contourf(xx, yy, Z, alpha=0.8)\n","scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_test, cmap=plt.cm.RdYlBu)\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","\n","# Add a legend\n","handles, labels = scatter.legend_elements()\n","plt.legend(handles, labels, title='Classes')\n","\n","plt.title('Test Decision Boundaries of Gaussian Bayes (2D)')\n","plt.show()"],"metadata":{"id":"kwPPUBzur_69"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 3 - Naive Bayes"],"metadata":{"id":"8-sAUwri1Gwr"}},{"cell_type":"markdown","source":["In this problem, you’ll implement a basic Naïve Bayes classifier, and use it to predict if a tweet is about cyberbullying or not. <br/>\n","We will have to classify sentences into 5 categories, <b>but could be any number.</b><br/>\n","The categories are {\"not bullying\", \"gender\", \"age\", \"religion\", \"ethnicity\"}.\n"],"metadata":{"id":"acuxlnhGm9BC"}},{"cell_type":"markdown","source":["import libarires"],"metadata":{"id":"BuhWspAV1n-l"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import math\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"AtBhju1K1qXh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the function. It reads all tweets from file and returns the following data structures: <br/>\n","•\ttexall - list of documents; each entry corresponds to a tweet which is list of words. <br/>\n","•\tlbAll list of tweets' labels.<br/>\n","•\tvoc - set of all distinct words in the file.<br/>\n","•\tcat - set of tweets categories.\n"],"metadata":{"id":"SwwN8oVTn-HO"}},{"cell_type":"code","source":["def readTrainData(file_name):\n","  df = pd.read_csv(file_name)\n","  # Implement here\n","  return texAll, lbAll, voc, cat"],"metadata":{"id":"_zZYY_brn9Vx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the function, which computes and returns the probabilities (on the train set):<br/>\n","- $P_w$ - a matrix of class-conditional probabilities, $p(x|w_i)$\n","- $P$ - a vector of class priors, $p(w_i)$\n","\n","Make sure you deal with the case of word that appears in voc but not in class $w$."],"metadata":{"id":"6xUbLY2Tol7u"}},{"cell_type":"code","source":["def learn_NB_text():\n","  # Implement here\n","\treturn Pw, P"],"metadata":{"id":"uYiXzZCIow9p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement fhe function that classifies all tweets from the test set and computes the success rate.<br/>\n","Iterate over all tweets of test and for each tweet find the most probable category.\n","<br/><br/>\n","Note1: Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. <br/>Class with highest final un-normalized log probability score is still the most probable.\n"],"metadata":{"id":"SkmMTneCpTm-"}},{"cell_type":"code","source":["def ClassifyNB_text(Pw, P):\n","\t# Implement here"],"metadata":{"id":"-epXTXF5EHtx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Read the files"],"metadata":{"id":"LMZXLXTbp84V"}},{"cell_type":"code","source":["TRAIN_FILE = 'https://sharon.srworkspace.com/ml/datasets/hw1/cyber_train.csv'\n","TEST_FILE = 'https://sharon.srworkspace.com/ml/datasets/hw1/cyber_test.csv'\n","\n","texAll_train, lblAll_train, voc, cat = readTrainData(TRAIN_FILE)\n","\n","# cats must be the same at train and test\n","# voc of test is irrelevant - we already trained on other voc.\n","texAll_test, lblAll_test, _, __ = readTrainData(TEST_FILE)"],"metadata":{"id":"wZa-wSjvZK2s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train the model, classify it on the test and report the success rate"],"metadata":{"id":"m1ss4d_dqyk5"}},{"cell_type":"code","source":["Pw, P = learn_NB_text()\n","sum_right = ClassifyNB_text(Pw, P)\n","print(sum_right)"],"metadata":{"id":"u5_sN3V-GOah"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 4 - KNN"],"metadata":{"id":"MsyhpIFXgWbo"}},{"cell_type":"markdown","source":["You want to detect types into 6 types of stars by measuring their properties. <br/> NASA gave you their dataset, including temperature, color, Spectral_Class and more. <br/> In addition, you aim to compare different distance metric to determine which one is the best for this data."],"metadata":{"id":"DOygDm7FrhWB"}},{"cell_type":"markdown","source":["import libaries"],"metadata":{"id":"jI5R52AwqOdA"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"],"metadata":{"id":"IrdvlliQgXpE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the data, print the first three rows<br/>\n","https://sharon.srworkspace.com/ml/datasets/hw1/Stars.csv"],"metadata":{"id":"YMt83YEaqPqf"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"rAGt7yWzn-pj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convert categorial features to discerete values"],"metadata":{"id":"RMJVaS8DqRBB"}},{"cell_type":"code","source":["colors = df['Color'].unique()\n","for idx, color in enumerate(colors):\n","  df['Color'] = df['Color'].replace({color: idx})\n","\n","spec_class = df['Spectral_Class'].unique()\n","for idx, spec in enumerate(spec_class):\n","  df['Spectral_Class'] = df['Spectral_Class'].replace({spec: idx})\n","df.head(3)"],"metadata":{"id":"nlPbLdxqooAy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check the correlation matrix between the features. <br/>\n","Which distance metric do you expect to work better: Euclidean distance, of the Mahalanobis distance? <br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"685jpA45qS_D"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"vrb2NRL6p6gC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split the data to 80% train and 20% test, with random state 21. <br/>\n","Make sure to maintain the dataset balanced, using stratify=y, in train_test_split method. <br/> You can check the balance using df.value_counts()."],"metadata":{"id":"4akX_fZnunkb"}},{"cell_type":"code","source":["# Implement here"],"metadata":{"id":"pzBXvhu9ukzG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the functions 'Euclidian', 'Manhattan'. <br/>\n","Those functions get train and test datasets and return distance metric, sized mxn (where m is the number of samples in test and n is the number of samples in train).<br/>\n","Reminder: Manhattan distance is $d(x,y)=\\sum_{i=1}^d |x_i-y_i|$, d is the features number.\n"],"metadata":{"id":"CfUnWaYiu59l"}},{"cell_type":"code","source":["def Euclidean(test, data):\n","  # Implement here\n","\n","def Manhattan(test, data):\n","  # Implement here\n","\n","def Mahalanobis(test, data):\n","  distances = np.zeros((test.shape[0], data.shape[0]))\n","  covariance_matrix_data = np.cov(data, rowvar=False)\n","\n","  # Calculate the Mahalanobis distances\n","  for i in range(test.shape[0]):\n","      for j in range(data.shape[0]):\n","          diff =  test[i] - data[j]\n","          distances[i, j] = np.sqrt(np.dot(np.dot(diff, np.linalg.inv(covariance_matrix_data)), diff.T))\n","  return distances"],"metadata":{"id":"Werkv3zoztln"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the function kNN_classify that returns array sized m, which are the predictions for the m test samples."],"metadata":{"id":"_U-RaTeOvYOe"}},{"cell_type":"code","source":["def kNN_classify(data, labels, test, k, metric='Euclidian'):\n","  arguments = (test, data)\n","  distances = eval(f'{metric}(*arguments)')   #returns np[][] |test| X |data| by the given metric.\n","  # Implement here"],"metadata":{"id":"1wvWALopvZd5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Look at the plots for different k values and compare those metrics."],"metadata":{"id":"oBOw9dqxwRLm"}},{"cell_type":"code","source":["metrics = ['Euclidean', 'Manhattan']\n","fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n","\n","for idx, metric in enumerate(metrics):\n","  # Plot data points and fitting line for Ordinary Least Squares\n","\n","  ks = np.arange(1, 20, 2)\n","  accs = []\n","  for k in ks:\n","    c = kNN_classify(X_train, y_train, X_test, k, metric)\n","    accs.append()   # Implement here\n","\n","  axs[idx // 2, idx % 2].plot(ks, accs, color='red')\n","  axs[idx // 2, idx % 2].set_xlabel('k')\n","  axs[idx // 2, idx % 2].set_ylabel('accuracy')\n","  axs[idx // 2, idx % 2].set_title(metric)\n","  axs[idx // 2, idx % 2].set_xticks(ks)\n","plt.show()"],"metadata":{"id":"Wp_CDXB4whVs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The Mahalanobis distance metric is already implemented to you. <br/>Run the following code and answer: Which gives better accuracy?<br/>\n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"84qffnSzw8-n"}},{"cell_type":"code","source":["ks = np.arange(1, 20, 2)\n","accs = []\n","for k in ks:\n","  c = kNN_classify(X_train, y_train, X_test, k, 'Mahalanobis')\n","  accs.append()   # Implement here\n","\n","axs[1, 0].plot(ks, accs, color='red')\n","axs[1, 0].set_xlabel('k')\n","axs[1, 0].set_ylabel('accuracy')\n","axs[1, 0].set_title('Mahalanobis')\n","axs[1, 0].set_xticks(ks)\n","plt.show()"],"metadata":{"id":"DILox6RB0P0T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You forgot to use scaling! Based on the feature densities (as done in Q3), determine which scaler should we use and perform it."],"metadata":{"id":"hFC4awK013TM"}},{"cell_type":"code","source":["# Peform scaling"],"metadata":{"id":"91ilngOy2SZY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Look at the plots. Did it help the accuracies? Try to justify the change in the hybrid metric.  \n","<font color='red'>Write here your answer and explain it</font>"],"metadata":{"id":"Cmgqs5Wz2ebe"}},{"cell_type":"code","source":["metrics = ['Euclidean', 'Manhattan', 'Mahalanobis', 'Hybrid']\n","fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n","\n","for idx, metric in enumerate(metrics):\n","  # Plot data points and fitting line for Ordinary Least Squares\n","\n","  ks = np.arange(1, 20, 2)\n","  accs = []\n","  for k in ks:\n","    c = kNN_classify(X_train_scaled, y_train, X_test_scaled, k, metric)\n","    accs.append()   # Implement here\n","\n","  axs[idx // 2, idx % 2].plot(ks, accs, color='red')\n","  axs[idx // 2, idx % 2].set_xlabel('k')\n","  axs[idx // 2, idx % 2].set_ylabel('accuracy')\n","  axs[idx // 2, idx % 2].set_title(metric)\n","  axs[idx // 2, idx % 2].set_xticks(ks)\n","plt.show()"],"metadata":{"id":"yIDpsFX42T1I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Thats it!\n","If you choose to continue, 5 pts bonus!"],"metadata":{"id":"rv1B0yktXJi7"}},{"cell_type":"markdown","source":["We want to use advanced distance metric: <br/>\n","$$d_{\\text{hybrid}}=d_{\\text{mahalanobis}}+0.5*d_{\\text{cosine}}$$\n","Implement the function 'Hybrid', which returns the distance metric, as defined above.\n"],"metadata":{"id":"QrO8cYB80mFF"}},{"cell_type":"code","source":["def Cosine(test, data):\n","  distances = np.zeros((test.shape[0], data.shape[0]))\n","\n","  # Calculate the Cosine distances\n","  for i in range(test.shape[0]):\n","      for j in range(data.shape[0]):\n","          distances[i, j] = 1 - np.sum(test[i] * data[j]) / (np.linalg.norm(test[i]) * np.linalg.norm(data[j]))\n","  return distances\n","\n","def Hybrid(test, data):\n","  # Implement here, 1-3 lines"],"metadata":{"id":"ulkuAChd0mRZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Look at the plot. Does it perform better than the previous metrics?"],"metadata":{"id":"M-o3iVEF1xc-"}},{"cell_type":"code","source":["ks = np.arange(1, 20, 2)\n","accs = []\n","for k in ks:\n","  c = kNN_classify(X_train, y_train, X_test, k, 'Hybrid')\n","  accs.append()   # Implement here\n","\n","axs[1, 1].plot(ks, accs, color='red')\n","axs[1, 1].set_xlabel('k')\n","axs[1, 1].set_ylabel('accuracy')\n","axs[1, 1].set_title('Hybrid')\n","axs[1, 1].set_xticks(ks)\n","plt.show()"],"metadata":{"id":"53-218RYr_2J"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["prp9oMrgRud7","8-sAUwri1Gwr"],"mount_file_id":"10Aq7iM8jGdN7GozuaSoIs4Br7VZvdWbC","authorship_tag":"ABX9TyORD7Lnchim7ybrygDDUTKh"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}